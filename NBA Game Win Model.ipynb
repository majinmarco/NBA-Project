{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c59f349-e60b-489e-b96e-eb543501e113",
   "metadata": {},
   "source": [
    "# NBA Game Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45cc3ecb-4a86-42ef-ae72-61efe7e4b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries: API, data manipulation, plotting\n",
    "import nba_api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baeb33e4-922a-416b-a5b9-d41220284701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d7fbe75-7eb4-4146-9015-12d3023de688",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaguegames = leaguegamefinder.LeagueGameFinder().get_data_frames()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d47115ed-5ef5-4f9d-96bc-42eeedbd1088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 28)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaguegames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d43ab4bb-145b-4a74-9845-0488aa39c1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON_ID</th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>TEAM_NAME</th>\n",
       "      <th>GAME_ID</th>\n",
       "      <th>GAME_DATE</th>\n",
       "      <th>MATCHUP</th>\n",
       "      <th>WL</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>...</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PLUS_MINUS</th>\n",
       "      <th>HOME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>42022</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>0042200314</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>LAL vs. DEN</td>\n",
       "      <td>L</td>\n",
       "      <td>241</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>42022</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>0042200313</td>\n",
       "      <td>2023-05-20</td>\n",
       "      <td>LAL vs. DEN</td>\n",
       "      <td>L</td>\n",
       "      <td>242</td>\n",
       "      <td>108</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>42022</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>0042200312</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>LAL @ DEN</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>42022</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>0042200311</td>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>LAL @ DEN</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>42022</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>0042200236</td>\n",
       "      <td>2023-05-12</td>\n",
       "      <td>LAL vs. GSW</td>\n",
       "      <td>W</td>\n",
       "      <td>240</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29692</th>\n",
       "      <td>22018</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>1521800030</td>\n",
       "      <td>2018-07-08</td>\n",
       "      <td>LAL @ CHI</td>\n",
       "      <td>W</td>\n",
       "      <td>198</td>\n",
       "      <td>69</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29715</th>\n",
       "      <td>22018</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>1521800020</td>\n",
       "      <td>2018-07-07</td>\n",
       "      <td>LAL vs. PHI</td>\n",
       "      <td>W</td>\n",
       "      <td>201</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29778</th>\n",
       "      <td>22018</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>1321800005</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>LAL @ GSW</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29792</th>\n",
       "      <td>22018</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>1321800003</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>LAL vs. MIA</td>\n",
       "      <td>L</td>\n",
       "      <td>241</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29807</th>\n",
       "      <td>22018</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>1321800002</td>\n",
       "      <td>2018-07-02</td>\n",
       "      <td>LAL @ SAC</td>\n",
       "      <td>L</td>\n",
       "      <td>239</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEASON_ID     TEAM_ID TEAM_ABBREVIATION           TEAM_NAME     GAME_ID  \\\n",
       "118       42022  1610612747               LAL  Los Angeles Lakers  0042200314   \n",
       "133       42022  1610612747               LAL  Los Angeles Lakers  0042200313   \n",
       "146       42022  1610612747               LAL  Los Angeles Lakers  0042200312   \n",
       "156       42022  1610612747               LAL  Los Angeles Lakers  0042200311   \n",
       "226       42022  1610612747               LAL  Los Angeles Lakers  0042200236   \n",
       "...         ...         ...               ...                 ...         ...   \n",
       "29692     22018  1610612747               LAL  Los Angeles Lakers  1521800030   \n",
       "29715     22018  1610612747               LAL  Los Angeles Lakers  1521800020   \n",
       "29778     22018  1610612747               LAL  Los Angeles Lakers  1321800005   \n",
       "29792     22018  1610612747               LAL  Los Angeles Lakers  1321800003   \n",
       "29807     22018  1610612747               LAL  Los Angeles Lakers  1321800002   \n",
       "\n",
       "       GAME_DATE      MATCHUP WL  MIN  PTS  ...  OREB  DREB  REB  AST  STL  \\\n",
       "118   2023-05-22  LAL vs. DEN  L  241  111  ...     8    30   38   20    5   \n",
       "133   2023-05-20  LAL vs. DEN  L  242  108  ...    11    34   45   27    3   \n",
       "146   2023-05-18    LAL @ DEN  L  240  103  ...     4    36   40   26   10   \n",
       "156   2023-05-16    LAL @ DEN  L  240  126  ...     5    25   30   30    6   \n",
       "226   2023-05-12  LAL vs. GSW  W  240  122  ...     6    40   46   25    6   \n",
       "...          ...          ... ..  ...  ...  ...   ...   ...  ...  ...  ...   \n",
       "29692 2018-07-08    LAL @ CHI  W  198   69  ...    10    34   44   14    9   \n",
       "29715 2018-07-07  LAL vs. PHI  W  201   96  ...    12    32   44   17    6   \n",
       "29778 2018-07-05    LAL @ GSW  L  240   71  ...     9    38   47   16    6   \n",
       "29792 2018-07-03  LAL vs. MIA  L  241   74  ...    12    24   36   14    7   \n",
       "29807 2018-07-02    LAL @ SAC  L  239   93  ...     8    24   32   22    9   \n",
       "\n",
       "       BLK  TOV  PF  PLUS_MINUS  HOME  \n",
       "118      4    6  19        -2.0     1  \n",
       "133      2   12  18       -11.0     1  \n",
       "146      7   12  19        -5.0     0  \n",
       "156      4    7  21        -6.0     0  \n",
       "226      6    6  17        21.0     1  \n",
       "...    ...  ...  ..         ...   ...  \n",
       "29692    2    8  17         9.0     0  \n",
       "29715    4   19  23        17.0     1  \n",
       "29778    7   23  19        -6.0     0  \n",
       "29792    7   20  27       -15.0     1  \n",
       "29807    4   14  23        -5.0     0  \n",
       "\n",
       "[498 rows x 29 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaguegames['HOME'] = np.where(leaguegames['MATCHUP'].apply(lambda x: x[4:7]) == 'vs.', 1, 0)\n",
    "leaguegames['GAME_DATE'] = pd.to_datetime(leaguegames['GAME_DATE'])\n",
    "leaguegames[leaguegames['TEAM_ABBREVIATION'] == 'LAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3b6a8f8-7e82-4602-86b4-72a99aeaeed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON_ID_H</th>\n",
       "      <th>TEAM_ID_H</th>\n",
       "      <th>TEAM_ABBREVIATION_H</th>\n",
       "      <th>TEAM_NAME_H</th>\n",
       "      <th>GAME_ID</th>\n",
       "      <th>GAME_DATE_H</th>\n",
       "      <th>MATCHUP_H</th>\n",
       "      <th>WL_H</th>\n",
       "      <th>MIN_H</th>\n",
       "      <th>PTS_H</th>\n",
       "      <th>...</th>\n",
       "      <th>OREB_O</th>\n",
       "      <th>DREB_O</th>\n",
       "      <th>REB_O</th>\n",
       "      <th>AST_O</th>\n",
       "      <th>STL_O</th>\n",
       "      <th>BLK_O</th>\n",
       "      <th>TOV_O</th>\n",
       "      <th>PF_O</th>\n",
       "      <th>PLUS_MINUS_O</th>\n",
       "      <th>HOME_O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1610612761</td>\n",
       "      <td>TOR</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>0041800406</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>TOR @ GSW</td>\n",
       "      <td>W</td>\n",
       "      <td>241</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1610612761</td>\n",
       "      <td>TOR</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>0041800406</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>TOR @ GSW</td>\n",
       "      <td>W</td>\n",
       "      <td>241</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1610612744</td>\n",
       "      <td>GSW</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>0041800406</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>GSW vs. TOR</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1610612744</td>\n",
       "      <td>GSW</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "      <td>0041800406</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>GSW vs. TOR</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1610612761</td>\n",
       "      <td>TOR</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>0041800405</td>\n",
       "      <td>2019-06-10</td>\n",
       "      <td>TOR vs. GSW</td>\n",
       "      <td>L</td>\n",
       "      <td>238</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49341</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>200</td>\n",
       "      <td>SGC</td>\n",
       "      <td>76ers GC</td>\n",
       "      <td>1212200006</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>SGC @ HCG</td>\n",
       "      <td>W</td>\n",
       "      <td>240</td>\n",
       "      <td>74</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49342</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>205</td>\n",
       "      <td>CCG</td>\n",
       "      <td>Celtics Crossover Gaming</td>\n",
       "      <td>1212200004</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>CCG vs. HTG</td>\n",
       "      <td>W</td>\n",
       "      <td>240</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49343</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>205</td>\n",
       "      <td>CCG</td>\n",
       "      <td>Celtics Crossover Gaming</td>\n",
       "      <td>1212200004</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>CCG vs. HTG</td>\n",
       "      <td>W</td>\n",
       "      <td>240</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49344</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>209</td>\n",
       "      <td>HTG</td>\n",
       "      <td>Hawks Talon GC</td>\n",
       "      <td>1212200004</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>HTG @ CCG</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49345</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>209</td>\n",
       "      <td>HTG</td>\n",
       "      <td>Hawks Talon GC</td>\n",
       "      <td>1212200004</td>\n",
       "      <td>2022-04-05</td>\n",
       "      <td>HTG @ CCG</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-29.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49346 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SEASON_ID_H   TEAM_ID_H TEAM_ABBREVIATION_H                TEAM_NAME_H  \\\n",
       "0      2018-01-01  1610612761                 TOR            Toronto Raptors   \n",
       "1      2018-01-01  1610612761                 TOR            Toronto Raptors   \n",
       "2      2018-01-01  1610612744                 GSW      Golden State Warriors   \n",
       "3      2018-01-01  1610612744                 GSW      Golden State Warriors   \n",
       "4      2018-01-01  1610612761                 TOR            Toronto Raptors   \n",
       "...           ...         ...                 ...                        ...   \n",
       "49341  2022-01-01         200                 SGC                   76ers GC   \n",
       "49342  2022-01-01         205                 CCG   Celtics Crossover Gaming   \n",
       "49343  2022-01-01         205                 CCG   Celtics Crossover Gaming   \n",
       "49344  2022-01-01         209                 HTG             Hawks Talon GC   \n",
       "49345  2022-01-01         209                 HTG             Hawks Talon GC   \n",
       "\n",
       "          GAME_ID GAME_DATE_H    MATCHUP_H WL_H  MIN_H  PTS_H  ...  OREB_O  \\\n",
       "0      0041800406  2019-06-13    TOR @ GSW    W    241    114  ...      11   \n",
       "1      0041800406  2019-06-13    TOR @ GSW    W    241    114  ...      11   \n",
       "2      0041800406  2019-06-13  GSW vs. TOR    L    240    110  ...      11   \n",
       "3      0041800406  2019-06-13  GSW vs. TOR    L    240    110  ...      11   \n",
       "4      0041800405  2019-06-10  TOR vs. GSW    L    238    105  ...      13   \n",
       "...           ...         ...          ...  ...    ...    ...  ...     ...   \n",
       "49341  1212200006  2022-04-05    SGC @ HCG    W    240     74  ...       5   \n",
       "49342  1212200004  2022-04-05  CCG vs. HTG    W    240     80  ...       5   \n",
       "49343  1212200004  2022-04-05  CCG vs. HTG    W    240     80  ...       3   \n",
       "49344  1212200004  2022-04-05    HTG @ CCG    L    240     57  ...       5   \n",
       "49345  1212200004  2022-04-05    HTG @ CCG    L    240     57  ...       3   \n",
       "\n",
       "       DREB_O  REB_O  AST_O  STL_O  BLK_O  TOV_O  PF_O  PLUS_MINUS_O  HOME_O  \n",
       "0          28     39     25      8      2     12    23           4.0       0  \n",
       "1          31     42     28      9      6     16    23          -4.0       1  \n",
       "2          28     39     25      8      2     12    23           4.0       0  \n",
       "3          31     42     28      9      6     16    23          -4.0       1  \n",
       "4          30     43     19      6      5     13    19          -1.0       1  \n",
       "...       ...    ...    ...    ...    ...    ...   ...           ...     ...  \n",
       "49341      18     23     15      1      5      5     5          29.0       0  \n",
       "49342      11     16     24      4      3      1    12          29.0       1  \n",
       "49343       9     12     16      1      0      4    10         -29.0       0  \n",
       "49344      11     16     24      4      3      1    12          29.0       1  \n",
       "49345       9     12     16      1      0      4    10         -29.0       0  \n",
       "\n",
       "[49346 rows x 57 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_games(seasons: list):\n",
    "    leaguegames['SEASON_ID'] = leaguegames['SEASON_ID'].apply(lambda x: pd.to_datetime(x[1:], format = '%Y'))\n",
    "    \n",
    "    try:\n",
    "        szn_games = leaguegames[leaguegames['SEASON_ID'] == str(seasons[0])] # gotta change this so it can do season_id instead of dates-- figure out what\n",
    "        if len(seasons)>1:\n",
    "            for season in seasons[1:]:\n",
    "                szn_games = pd.concat([szn_games, leaguegames[leaguegames['SEASON_ID'] == str(season)]])\n",
    "    except:\n",
    "        print(\"Seasons not found.\")\n",
    "        return -1\n",
    "    \n",
    "    # Remove summer league games! (confusing and irritating for visualizations)\n",
    "    szn_games = szn_games[szn_games.GAME_DATE.dt.month != 7]\n",
    "    \n",
    "    # opposite team merging\n",
    "    team_games = szn_games[szn_games['TEAM_ABBREVIATION'] == szn_games['MATCHUP'].apply(lambda x: x[:3])]\n",
    "    o_games = szn_games[szn_games['TEAM_NAME'] != szn_games['MATCHUP'].apply(lambda x: x[:3])]\n",
    "    \n",
    "    return team_games.merge(on = 'GAME_ID', suffixes = ['_H', '_O'], right = o_games)\n",
    "    \n",
    "df = get_games([2018, 2019, 2020, 2021, 2022])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b57c67fa-6f00-47aa-ac94-d64ab3755e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49346 entries, 0 to 49345\n",
      "Data columns (total 57 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   SEASON_ID_H          49346 non-null  datetime64[ns]\n",
      " 1   TEAM_ID_H            49346 non-null  int64         \n",
      " 2   TEAM_ABBREVIATION_H  49346 non-null  object        \n",
      " 3   TEAM_NAME_H          49346 non-null  object        \n",
      " 4   GAME_ID              49346 non-null  object        \n",
      " 5   GAME_DATE_H          49346 non-null  datetime64[ns]\n",
      " 6   MATCHUP_H            49346 non-null  object        \n",
      " 7   WL_H                 46779 non-null  object        \n",
      " 8   MIN_H                49346 non-null  int64         \n",
      " 9   PTS_H                49346 non-null  int64         \n",
      " 10  FGM_H                49346 non-null  int64         \n",
      " 11  FGA_H                49346 non-null  int64         \n",
      " 12  FG_PCT_H             49338 non-null  float64       \n",
      " 13  FG3M_H               49346 non-null  int64         \n",
      " 14  FG3A_H               49346 non-null  int64         \n",
      " 15  FG3_PCT_H            49326 non-null  float64       \n",
      " 16  FTM_H                49346 non-null  int64         \n",
      " 17  FTA_H                49346 non-null  int64         \n",
      " 18  FT_PCT_H             45413 non-null  float64       \n",
      " 19  OREB_H               49346 non-null  int64         \n",
      " 20  DREB_H               49346 non-null  int64         \n",
      " 21  REB_H                49346 non-null  int64         \n",
      " 22  AST_H                49346 non-null  int64         \n",
      " 23  STL_H                49346 non-null  int64         \n",
      " 24  BLK_H                49346 non-null  int64         \n",
      " 25  TOV_H                49346 non-null  int64         \n",
      " 26  PF_H                 49346 non-null  int64         \n",
      " 27  PLUS_MINUS_H         49098 non-null  float64       \n",
      " 28  HOME_H               49346 non-null  int64         \n",
      " 29  SEASON_ID_O          49346 non-null  datetime64[ns]\n",
      " 30  TEAM_ID_O            49346 non-null  int64         \n",
      " 31  TEAM_ABBREVIATION_O  49346 non-null  object        \n",
      " 32  TEAM_NAME_O          49346 non-null  object        \n",
      " 33  GAME_DATE_O          49346 non-null  datetime64[ns]\n",
      " 34  MATCHUP_O            49346 non-null  object        \n",
      " 35  WL_O                 46779 non-null  object        \n",
      " 36  MIN_O                49346 non-null  int64         \n",
      " 37  PTS_O                49346 non-null  int64         \n",
      " 38  FGM_O                49346 non-null  int64         \n",
      " 39  FGA_O                49346 non-null  int64         \n",
      " 40  FG_PCT_O             49338 non-null  float64       \n",
      " 41  FG3M_O               49346 non-null  int64         \n",
      " 42  FG3A_O               49346 non-null  int64         \n",
      " 43  FG3_PCT_O            49326 non-null  float64       \n",
      " 44  FTM_O                49346 non-null  int64         \n",
      " 45  FTA_O                49346 non-null  int64         \n",
      " 46  FT_PCT_O             45413 non-null  float64       \n",
      " 47  OREB_O               49346 non-null  int64         \n",
      " 48  DREB_O               49346 non-null  int64         \n",
      " 49  REB_O                49346 non-null  int64         \n",
      " 50  AST_O                49346 non-null  int64         \n",
      " 51  STL_O                49346 non-null  int64         \n",
      " 52  BLK_O                49346 non-null  int64         \n",
      " 53  TOV_O                49346 non-null  int64         \n",
      " 54  PF_O                 49346 non-null  int64         \n",
      " 55  PLUS_MINUS_O         49098 non-null  float64       \n",
      " 56  HOME_O               49346 non-null  int64         \n",
      "dtypes: datetime64[ns](4), float64(8), int64(36), object(9)\n",
      "memory usage: 21.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f5f1620-0101-4eea-ad92-468db86e6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['MONTH'] = df['GAME_DATE_H'].dt.month\n",
    "df['SEASON'] = df['SEASON_ID_H']\n",
    "#df['YEAR'] = df['GAME_DATE_H'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8b4c233-c4af-4830-b9a2-499f2ba6e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['SEASON_ID_O', 'SEASON_ID_H', 'GAME_DATE_O', 'TEAM_ID_H', 'TEAM_ID_O', \n",
    "              'TEAM_NAME_H', 'TEAM_NAME_O', 'MATCHUP_O', 'WL_O', 'MIN_O', 'MIN_H', \n",
    "              'GAME_ID', 'TEAM_ABBREVIATION_O'], axis = 1)\n",
    "df = df.rename({'GAME_DATE_H':'GAME_DATE'}, axis = 1)\n",
    "df = df.set_index('GAME_DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f9fd3-18bd-4cbf-aa71-5d8fcb17bffb",
   "metadata": {},
   "source": [
    "# Handle Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51e932a6-f16b-4373-809c-c4dcc40e342f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TEAM_ABBREVIATION_H       0\n",
       "MATCHUP_H                 0\n",
       "WL_H                   2567\n",
       "PTS_H                     0\n",
       "FGM_H                     0\n",
       "FGA_H                     0\n",
       "FG_PCT_H                  8\n",
       "FG3M_H                    0\n",
       "FG3A_H                    0\n",
       "FG3_PCT_H                20\n",
       "FTM_H                     0\n",
       "FTA_H                     0\n",
       "FT_PCT_H               3933\n",
       "OREB_H                    0\n",
       "DREB_H                    0\n",
       "REB_H                     0\n",
       "AST_H                     0\n",
       "STL_H                     0\n",
       "BLK_H                     0\n",
       "TOV_H                     0\n",
       "PF_H                      0\n",
       "PLUS_MINUS_H            248\n",
       "HOME_H                    0\n",
       "PTS_O                     0\n",
       "FGM_O                     0\n",
       "FGA_O                     0\n",
       "FG_PCT_O                  8\n",
       "FG3M_O                    0\n",
       "FG3A_O                    0\n",
       "FG3_PCT_O                20\n",
       "FTM_O                     0\n",
       "FTA_O                     0\n",
       "FT_PCT_O               3933\n",
       "OREB_O                    0\n",
       "DREB_O                    0\n",
       "REB_O                     0\n",
       "AST_O                     0\n",
       "STL_O                     0\n",
       "BLK_O                     0\n",
       "TOV_O                     0\n",
       "PF_O                      0\n",
       "PLUS_MINUS_O            248\n",
       "HOME_O                    0\n",
       "SEASON                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ad824fa-544e-42f4-88c8-a4013b426ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TEAM_ABBREVIATION_H       0\n",
       "MATCHUP_H                 0\n",
       "WL_H                      0\n",
       "PTS_H                     0\n",
       "FGM_H                     0\n",
       "FGA_H                     0\n",
       "FG_PCT_H                  0\n",
       "FG3M_H                    0\n",
       "FG3A_H                    0\n",
       "FG3_PCT_H                 6\n",
       "FTM_H                     0\n",
       "FTA_H                     0\n",
       "FT_PCT_H               1378\n",
       "OREB_H                    0\n",
       "DREB_H                    0\n",
       "REB_H                     0\n",
       "AST_H                     0\n",
       "STL_H                     0\n",
       "BLK_H                     0\n",
       "TOV_H                     0\n",
       "PF_H                      0\n",
       "PLUS_MINUS_H            248\n",
       "HOME_H                    0\n",
       "PTS_O                     0\n",
       "FGM_O                     0\n",
       "FGA_O                     0\n",
       "FG_PCT_O                  0\n",
       "FG3M_O                    0\n",
       "FG3A_O                    0\n",
       "FG3_PCT_O                 6\n",
       "FTM_O                     0\n",
       "FTA_O                     0\n",
       "FT_PCT_O               1380\n",
       "OREB_O                    0\n",
       "DREB_O                    0\n",
       "REB_O                     0\n",
       "AST_O                     0\n",
       "STL_O                     0\n",
       "BLK_O                     0\n",
       "TOV_O                     0\n",
       "PF_O                      0\n",
       "PLUS_MINUS_O            248\n",
       "HOME_O                    0\n",
       "SEASON                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['WL_H'], axis = 0)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57bbd674-795b-4994-9357-9b78adc0336a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1378\n",
       "Name: FTA_H, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FTA_H'][np.isnan(df['FT_PCT_H'])].value_counts() # these are all 0 sheesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90c60f6f-db5b-4cd0-a4f2-b88cf66f3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['FT_PCT_H'].fillna(value = (df['FTM_H']/df['FTA_H']), inplace = True)\n",
    "# df['FT_PCT_O'].fillna(value = (df['FTM_O']/df['FTA_O']), inplace = True)\n",
    "\n",
    "df['FT_PCT_H'] = df['FT_PCT_H'].fillna(0)\n",
    "df['FT_PCT_O'] = df['FT_PCT_O'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9093d0f0-2c28-4c47-88fb-d9ceff23d0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TEAM_ABBREVIATION_H    0\n",
       "MATCHUP_H              0\n",
       "WL_H                   0\n",
       "PTS_H                  0\n",
       "FGM_H                  0\n",
       "FGA_H                  0\n",
       "FG_PCT_H               0\n",
       "FG3M_H                 0\n",
       "FG3A_H                 0\n",
       "FG3_PCT_H              4\n",
       "FTM_H                  0\n",
       "FTA_H                  0\n",
       "FT_PCT_H               0\n",
       "OREB_H                 0\n",
       "DREB_H                 0\n",
       "REB_H                  0\n",
       "AST_H                  0\n",
       "STL_H                  0\n",
       "BLK_H                  0\n",
       "TOV_H                  0\n",
       "PF_H                   0\n",
       "PLUS_MINUS_H           0\n",
       "HOME_H                 0\n",
       "PTS_O                  0\n",
       "FGM_O                  0\n",
       "FGA_O                  0\n",
       "FG_PCT_O               0\n",
       "FG3M_O                 0\n",
       "FG3A_O                 0\n",
       "FG3_PCT_O              4\n",
       "FTM_O                  0\n",
       "FTA_O                  0\n",
       "FT_PCT_O               0\n",
       "OREB_O                 0\n",
       "DREB_O                 0\n",
       "REB_O                  0\n",
       "AST_O                  0\n",
       "STL_O                  0\n",
       "BLK_O                  0\n",
       "TOV_O                  0\n",
       "PF_O                   0\n",
       "PLUS_MINUS_O           0\n",
       "HOME_O                 0\n",
       "SEASON                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset = ['PLUS_MINUS_H', 'PLUS_MINUS_O'], axis = 0)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9cf3d-9283-4410-a52d-a7621d548cbb",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "33cdf67d-c0ac-410a-9e0f-50b280679d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possessions: 0.96*[(Field Goal Attempts)+(Turnovers)+0.44*(Free Throw Attempts)-(Offensive Rebounds)]\n",
    "df['POSS_H'] = 0.96*(df['FGA_H'] + df['TOV_H'] + 0.44*(df['FTA_H']) - df['OREB_H'])\n",
    "df['POSS_O'] = 0.96*(df['FGA_O'] + df['TOV_O'] + 0.44*(df['FTA_O']) - df['OREB_O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe0553cc-8806-4127-b52c-c92b9fab541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFRTG 100*((Points)/(POSS))\n",
    "df['OFFRTG_H'] = 100*(df.PTS_H/df.POSS_H)\n",
    "df['OFFRTG_O'] = 100*(df.PTS_O/df.POSS_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ace223cf-6001-453a-9697-f44513e06943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFRTG 100*((Opp Points)/(Opp POSS))\n",
    "df['DEFRTG_H'] = 100*(df.PTS_O/df.POSS_O)\n",
    "df['DEFRTG_O'] = 100*(df.PTS_H/df.POSS_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24de0bfc-726e-4bac-ac68-94d38b7a8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether the team was playing home\n",
    "df['HOME_H'] = np.where(df['MATCHUP_H'].apply(lambda x: x[4:7]) == 'vs.', 1, 0)\n",
    "df = df.drop('MATCHUP_H', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2185f3b3-5bd2-4dc8-9a0a-be762951dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether W was home\n",
    "df['HOME_W_H'] = np.where((df.HOME_H == 1) & (df.WL_H == 1), 1, 0)\n",
    "df['HOME_W_O'] = np.where((df.HOME_H == 0) & (df.WL_H == 0), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a88f0f50-fe13-498a-9ece-d57dc5a3cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective Field Goal %\n",
    "df['EFG_H'] = (df.FGM_H+(0.5*df.FG3M_H))/(df.FGA_H+df.FG3A_H)\n",
    "df['EFG_O'] = (df.FGM_O+(0.5*df.FG3M_O))/(df.FGA_O+df.FG3A_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "770a78f6-c4bc-4066-98bb-7e3bdf49bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Shooting %\n",
    "df['TS_H'] = (df.PTS_H/(2*(df.FGA_H+(0.44*df.FTA_H))))*100\n",
    "df['TS_O'] = (df.PTS_O/(2*(df.FGA_O+(0.44*df.FTA_O))))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4420a55b-c255-43f3-b899-b8b112ba679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WL_H'] = df['WL_H'].apply(lambda x: 1 if x=='W' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b8f18925-bce4-40f5-8bab-faa344ce7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('SEASON', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c655df3d-8450-4278-93a2-40685d7fe243",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_move = df.pop('WL_H')\n",
    "df.insert(len(df.columns), \"WL_H\", column_to_move) # move to last place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7ca7f33-5e41-43d8-82dc-a5cab7d2308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46531"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5dae672-45e8-4338-a3e6-4c06669c4c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46531, 54)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d246f2d2-1004-4672-9ff5-a5dca1d49659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM_ABBREVIATION_H</th>\n",
       "      <th>PTS_H</th>\n",
       "      <th>FGM_H</th>\n",
       "      <th>FGA_H</th>\n",
       "      <th>FG_PCT_H</th>\n",
       "      <th>FG3M_H</th>\n",
       "      <th>FG3A_H</th>\n",
       "      <th>FG3_PCT_H</th>\n",
       "      <th>FTM_H</th>\n",
       "      <th>FTA_H</th>\n",
       "      <th>...</th>\n",
       "      <th>OFFRTG_O</th>\n",
       "      <th>DEFRTG_H</th>\n",
       "      <th>DEFRTG_O</th>\n",
       "      <th>HOME_W_H</th>\n",
       "      <th>HOME_W_O</th>\n",
       "      <th>EFG_H</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>TS_H</th>\n",
       "      <th>TS_O</th>\n",
       "      <th>WL_H</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAME_DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>TOR</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>82</td>\n",
       "      <td>0.476</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>0.394</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>TOR</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>82</td>\n",
       "      <td>0.476</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>0.394</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>GSW</td>\n",
       "      <td>110</td>\n",
       "      <td>39</td>\n",
       "      <td>80</td>\n",
       "      <td>0.488</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>0.355</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>GSW</td>\n",
       "      <td>110</td>\n",
       "      <td>39</td>\n",
       "      <td>80</td>\n",
       "      <td>0.488</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>0.355</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>TOR</td>\n",
       "      <td>105</td>\n",
       "      <td>38</td>\n",
       "      <td>85</td>\n",
       "      <td>0.447</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.250</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>112.897399</td>\n",
       "      <td>112.897399</td>\n",
       "      <td>112.897399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>54.190751</td>\n",
       "      <td>54.190751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>SGC</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>0.682</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0.524</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>170.086790</td>\n",
       "      <td>170.086790</td>\n",
       "      <td>170.086790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>0.546154</td>\n",
       "      <td>81.641659</td>\n",
       "      <td>81.641659</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>CCG</td>\n",
       "      <td>80</td>\n",
       "      <td>31</td>\n",
       "      <td>47</td>\n",
       "      <td>0.660</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>0.556</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>188.026474</td>\n",
       "      <td>188.026474</td>\n",
       "      <td>188.026474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>82.781457</td>\n",
       "      <td>82.781457</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>CCG</td>\n",
       "      <td>80</td>\n",
       "      <td>31</td>\n",
       "      <td>47</td>\n",
       "      <td>0.660</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>0.556</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>141.369048</td>\n",
       "      <td>141.369048</td>\n",
       "      <td>188.026474</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>82.781457</td>\n",
       "      <td>69.512195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>HTG</td>\n",
       "      <td>57</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.585</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>188.026474</td>\n",
       "      <td>188.026474</td>\n",
       "      <td>141.369048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>69.512195</td>\n",
       "      <td>82.781457</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-05</th>\n",
       "      <td>HTG</td>\n",
       "      <td>57</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.585</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>141.369048</td>\n",
       "      <td>141.369048</td>\n",
       "      <td>141.369048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>0.483051</td>\n",
       "      <td>69.512195</td>\n",
       "      <td>69.512195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46531 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEAM_ABBREVIATION_H  PTS_H  FGM_H  FGA_H  FG_PCT_H  FG3M_H  FG3A_H  \\\n",
       "GAME_DATE                                                                       \n",
       "2019-06-13                 TOR    114     39     82     0.476      13      33   \n",
       "2019-06-13                 TOR    114     39     82     0.476      13      33   \n",
       "2019-06-13                 GSW    110     39     80     0.488      11      31   \n",
       "2019-06-13                 GSW    110     39     80     0.488      11      31   \n",
       "2019-06-10                 TOR    105     38     85     0.447       8      32   \n",
       "...                        ...    ...    ...    ...       ...     ...     ...   \n",
       "2022-04-05                 SGC     74     30     44     0.682      11      21   \n",
       "2022-04-05                 CCG     80     31     47     0.660      15      27   \n",
       "2022-04-05                 CCG     80     31     47     0.660      15      27   \n",
       "2022-04-05                 HTG     57     24     41     0.585       9      18   \n",
       "2022-04-05                 HTG     57     24     41     0.585       9      18   \n",
       "\n",
       "            FG3_PCT_H  FTM_H  FTA_H  ...    OFFRTG_O    DEFRTG_H    DEFRTG_O  \\\n",
       "GAME_DATE                            ...                                       \n",
       "2019-06-13      0.394     23     29  ...  124.007937  124.007937  124.007937   \n",
       "2019-06-13      0.394     23     29  ...  116.683639  116.683639  124.007937   \n",
       "2019-06-13      0.355     21     30  ...  124.007937  124.007937  116.683639   \n",
       "2019-06-13      0.355     21     30  ...  116.683639  116.683639  116.683639   \n",
       "2019-06-10      0.250     21     27  ...  112.897399  112.897399  112.897399   \n",
       "...               ...    ...    ...  ...         ...         ...         ...   \n",
       "2022-04-05      0.524      3      3  ...  170.086790  170.086790  170.086790   \n",
       "2022-04-05      0.556      3      3  ...  188.026474  188.026474  188.026474   \n",
       "2022-04-05      0.556      3      3  ...  141.369048  141.369048  188.026474   \n",
       "2022-04-05      0.500      0      0  ...  188.026474  188.026474  141.369048   \n",
       "2022-04-05      0.500      0      0  ...  141.369048  141.369048  141.369048   \n",
       "\n",
       "            HOME_W_H  HOME_W_O     EFG_H     EFG_O       TS_H       TS_O  WL_H  \n",
       "GAME_DATE                                                                       \n",
       "2019-06-13         0         0  0.395652  0.395652  60.151963  60.151963     1  \n",
       "2019-06-13         0         0  0.395652  0.400901  60.151963  59.012876     1  \n",
       "2019-06-13         0         0  0.400901  0.395652  59.012876  60.151963     0  \n",
       "2019-06-13         0         0  0.400901  0.400901  59.012876  59.012876     0  \n",
       "2019-06-10         0         0  0.358974  0.358974  54.190751  54.190751     0  \n",
       "...              ...       ...       ...       ...        ...        ...   ...  \n",
       "2022-04-05         0         0  0.546154  0.546154  81.641659  81.641659     1  \n",
       "2022-04-05         0         0  0.520270  0.520270  82.781457  82.781457     1  \n",
       "2022-04-05         0         0  0.520270  0.483051  82.781457  69.512195     1  \n",
       "2022-04-05         0         0  0.483051  0.520270  69.512195  82.781457     0  \n",
       "2022-04-05         0         0  0.483051  0.483051  69.512195  69.512195     0  \n",
       "\n",
       "[46531 rows x 54 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32dcf77d-2b00-4398-8223-5a9ba73562c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('NBA_Model_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c55aef5-7f8e-4b5b-8a6a-7f9cb4345851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nba main league team abbreviations\n",
    "\n",
    "team_abv = pd.read_csv('Team_abv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1ffb755-8dde-4291-adf9-0efa0df45c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abbreviation/\\nAcronym</th>\n",
       "      <th>Franchise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL</td>\n",
       "      <td>Atlanta Hawks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOS</td>\n",
       "      <td>Boston Celtics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BKN</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHA</td>\n",
       "      <td>Charlotte Hornets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHI</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CLE</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DAL</td>\n",
       "      <td>Dallas Mavericks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DEN</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DET</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GSW</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HOU</td>\n",
       "      <td>Houston Rockets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IND</td>\n",
       "      <td>Indiana Pacers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LAC</td>\n",
       "      <td>Los Angeles Clippers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MEM</td>\n",
       "      <td>Memphis Grizzlies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MIA</td>\n",
       "      <td>Miami Heat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MIL</td>\n",
       "      <td>Milwaukee Bucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MIN</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NOP</td>\n",
       "      <td>New Orleans Pelicans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NYK</td>\n",
       "      <td>New York Knicks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>OKC</td>\n",
       "      <td>Oklahoma City Thunder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ORL</td>\n",
       "      <td>Orlando Magic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>PHI</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>PHX</td>\n",
       "      <td>Phoenix Suns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>POR</td>\n",
       "      <td>Portland Trail Blazers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SAC</td>\n",
       "      <td>Sacramento Kings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SAS</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TOR</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>UTA</td>\n",
       "      <td>Utah Jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>WAS</td>\n",
       "      <td>Washington Wizards</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Abbreviation/\\nAcronym               Franchise\n",
       "0                     ATL           Atlanta Hawks\n",
       "1                     BOS          Boston Celtics\n",
       "2                     BKN           Brooklyn Nets\n",
       "3                     CHA       Charlotte Hornets\n",
       "4                     CHI           Chicago Bulls\n",
       "5                     CLE     Cleveland Cavaliers\n",
       "6                     DAL        Dallas Mavericks\n",
       "7                     DEN          Denver Nuggets\n",
       "8                     DET         Detroit Pistons\n",
       "9                     GSW   Golden State Warriors\n",
       "10                    HOU         Houston Rockets\n",
       "11                    IND          Indiana Pacers\n",
       "12                    LAC    Los Angeles Clippers\n",
       "13                    LAL      Los Angeles Lakers\n",
       "14                    MEM       Memphis Grizzlies\n",
       "15                    MIA              Miami Heat\n",
       "16                    MIL         Milwaukee Bucks\n",
       "17                    MIN  Minnesota Timberwolves\n",
       "18                    NOP    New Orleans Pelicans\n",
       "19                    NYK         New York Knicks\n",
       "20                    OKC   Oklahoma City Thunder\n",
       "21                    ORL           Orlando Magic\n",
       "22                    PHI      Philadelphia 76ers\n",
       "23                    PHX            Phoenix Suns\n",
       "24                    POR  Portland Trail Blazers\n",
       "25                    SAC        Sacramento Kings\n",
       "26                    SAS       San Antonio Spurs\n",
       "27                    TOR         Toronto Raptors\n",
       "28                    UTA               Utah Jazz\n",
       "29                    WAS      Washington Wizards"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_abv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1140961c-69ca-4a12-8fb1-8b3f581ae94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ATL\n",
       "1     BOS\n",
       "2     BKN\n",
       "3     CHA\n",
       "4     CHI\n",
       "5     CLE\n",
       "6     DAL\n",
       "7     DEN\n",
       "8     DET\n",
       "9     GSW\n",
       "10    HOU\n",
       "11    IND\n",
       "12    LAC\n",
       "13    LAL\n",
       "14    MEM\n",
       "15    MIA\n",
       "16    MIL\n",
       "17    MIN\n",
       "18    NOP\n",
       "19    NYK\n",
       "20    OKC\n",
       "21    ORL\n",
       "22    PHI\n",
       "23    PHX\n",
       "24    POR\n",
       "25    SAC\n",
       "26    SAS\n",
       "27    TOR\n",
       "28    UTA\n",
       "29    WAS\n",
       "Name: Abbreviation/\\nAcronym, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teams = team_abv['Abbreviation/\\nAcronym']\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8d58181-b212-42f9-a859-673e9fc0a7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM_ABBREVIATION_H</th>\n",
       "      <th>PTS_H</th>\n",
       "      <th>FGM_H</th>\n",
       "      <th>FGA_H</th>\n",
       "      <th>FG_PCT_H</th>\n",
       "      <th>FG3M_H</th>\n",
       "      <th>FG3A_H</th>\n",
       "      <th>FG3_PCT_H</th>\n",
       "      <th>FTM_H</th>\n",
       "      <th>FTA_H</th>\n",
       "      <th>...</th>\n",
       "      <th>OFFRTG_O</th>\n",
       "      <th>DEFRTG_H</th>\n",
       "      <th>DEFRTG_O</th>\n",
       "      <th>HOME_W_H</th>\n",
       "      <th>HOME_W_O</th>\n",
       "      <th>EFG_H</th>\n",
       "      <th>EFG_O</th>\n",
       "      <th>TS_H</th>\n",
       "      <th>TS_O</th>\n",
       "      <th>WL_H</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAME_DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>TOR</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>82</td>\n",
       "      <td>0.476</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>0.394</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>TOR</td>\n",
       "      <td>114</td>\n",
       "      <td>39</td>\n",
       "      <td>82</td>\n",
       "      <td>0.476</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>0.394</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>GSW</td>\n",
       "      <td>110</td>\n",
       "      <td>39</td>\n",
       "      <td>80</td>\n",
       "      <td>0.488</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>0.355</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>124.007937</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>60.151963</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-13</th>\n",
       "      <td>GSW</td>\n",
       "      <td>110</td>\n",
       "      <td>39</td>\n",
       "      <td>80</td>\n",
       "      <td>0.488</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>0.355</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>116.683639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>59.012876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-10</th>\n",
       "      <td>TOR</td>\n",
       "      <td>105</td>\n",
       "      <td>38</td>\n",
       "      <td>85</td>\n",
       "      <td>0.447</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.250</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>112.897399</td>\n",
       "      <td>112.897399</td>\n",
       "      <td>112.897399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>54.190751</td>\n",
       "      <td>54.190751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-25</th>\n",
       "      <td>DAL</td>\n",
       "      <td>92</td>\n",
       "      <td>35</td>\n",
       "      <td>67</td>\n",
       "      <td>0.522</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0.524</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>118.546924</td>\n",
       "      <td>118.546924</td>\n",
       "      <td>118.546924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460227</td>\n",
       "      <td>0.460227</td>\n",
       "      <td>64.031180</td>\n",
       "      <td>64.031180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-24</th>\n",
       "      <td>ATL</td>\n",
       "      <td>88</td>\n",
       "      <td>31</td>\n",
       "      <td>73</td>\n",
       "      <td>0.425</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>0.609</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>112.502045</td>\n",
       "      <td>112.502045</td>\n",
       "      <td>112.502045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>54.671968</td>\n",
       "      <td>54.671968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-24</th>\n",
       "      <td>ATL</td>\n",
       "      <td>88</td>\n",
       "      <td>31</td>\n",
       "      <td>73</td>\n",
       "      <td>0.425</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>0.609</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>84.399953</td>\n",
       "      <td>84.399953</td>\n",
       "      <td>112.502045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>54.671968</td>\n",
       "      <td>40.041783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-24</th>\n",
       "      <td>WAS</td>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "      <td>80</td>\n",
       "      <td>0.325</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0.308</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>112.502045</td>\n",
       "      <td>112.502045</td>\n",
       "      <td>84.399953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>40.041783</td>\n",
       "      <td>54.671968</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-24</th>\n",
       "      <td>WAS</td>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "      <td>80</td>\n",
       "      <td>0.325</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0.308</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>84.399953</td>\n",
       "      <td>84.399953</td>\n",
       "      <td>84.399953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>40.041783</td>\n",
       "      <td>40.041783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28196 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TEAM_ABBREVIATION_H  PTS_H  FGM_H  FGA_H  FG_PCT_H  FG3M_H  FG3A_H  \\\n",
       "GAME_DATE                                                                       \n",
       "2019-06-13                 TOR    114     39     82     0.476      13      33   \n",
       "2019-06-13                 TOR    114     39     82     0.476      13      33   \n",
       "2019-06-13                 GSW    110     39     80     0.488      11      31   \n",
       "2019-06-13                 GSW    110     39     80     0.488      11      31   \n",
       "2019-06-10                 TOR    105     38     85     0.447       8      32   \n",
       "...                        ...    ...    ...    ...       ...     ...     ...   \n",
       "2022-04-25                 DAL     92     35     67     0.522      11      21   \n",
       "2022-04-24                 ATL     88     31     73     0.425      14      23   \n",
       "2022-04-24                 ATL     88     31     73     0.425      14      23   \n",
       "2022-04-24                 WAS     69     26     80     0.325       8      26   \n",
       "2022-04-24                 WAS     69     26     80     0.325       8      26   \n",
       "\n",
       "            FG3_PCT_H  FTM_H  FTA_H  ...    OFFRTG_O    DEFRTG_H    DEFRTG_O  \\\n",
       "GAME_DATE                            ...                                       \n",
       "2019-06-13      0.394     23     29  ...  124.007937  124.007937  124.007937   \n",
       "2019-06-13      0.394     23     29  ...  116.683639  116.683639  124.007937   \n",
       "2019-06-13      0.355     21     30  ...  124.007937  124.007937  116.683639   \n",
       "2019-06-13      0.355     21     30  ...  116.683639  116.683639  116.683639   \n",
       "2019-06-10      0.250     21     27  ...  112.897399  112.897399  112.897399   \n",
       "...               ...    ...    ...  ...         ...         ...         ...   \n",
       "2022-04-25      0.524     11     11  ...  118.546924  118.546924  118.546924   \n",
       "2022-04-24      0.609     12     17  ...  112.502045  112.502045  112.502045   \n",
       "2022-04-24      0.609     12     17  ...   84.399953   84.399953  112.502045   \n",
       "2022-04-24      0.308      9     14  ...  112.502045  112.502045   84.399953   \n",
       "2022-04-24      0.308      9     14  ...   84.399953   84.399953   84.399953   \n",
       "\n",
       "            HOME_W_H  HOME_W_O     EFG_H     EFG_O       TS_H       TS_O  WL_H  \n",
       "GAME_DATE                                                                       \n",
       "2019-06-13         0         0  0.395652  0.395652  60.151963  60.151963     1  \n",
       "2019-06-13         0         0  0.395652  0.400901  60.151963  59.012876     1  \n",
       "2019-06-13         0         0  0.400901  0.395652  59.012876  60.151963     0  \n",
       "2019-06-13         0         0  0.400901  0.400901  59.012876  59.012876     0  \n",
       "2019-06-10         0         0  0.358974  0.358974  54.190751  54.190751     0  \n",
       "...              ...       ...       ...       ...        ...        ...   ...  \n",
       "2022-04-25         0         0  0.460227  0.460227  64.031180  64.031180     1  \n",
       "2022-04-24         0         0  0.395833  0.395833  54.671968  54.671968     1  \n",
       "2022-04-24         0         0  0.395833  0.283019  54.671968  40.041783     1  \n",
       "2022-04-24         0         0  0.283019  0.395833  40.041783  54.671968     0  \n",
       "2022-04-24         0         0  0.283019  0.283019  40.041783  40.041783     0  \n",
       "\n",
       "[28196 rows x 54 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['TEAM_ABBREVIATION_H'].isin(teams)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f3d9e-ea86-4e0c-81ee-14e6056947a7",
   "metadata": {},
   "source": [
    "Got rid of over 25% of the data!!! Good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf59269-c242-4bf6-85f6-dca0c827316d",
   "metadata": {},
   "source": [
    "# Create RNN Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79d82b-d33b-418d-ad57-6285fdc458a2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb152151-012a-43d5-aa56-d326c23bb788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 8, 16)            3968      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 16)               400       \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,457\n",
      "Trainable params: 5,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, SimpleRNN, GRU, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model\n",
    "model.add(Bidirectional(LSTM(8, return_sequences=True, recurrent_dropout=0.2, activation='relu'), input_shape = (8, 53)))\n",
    "model.add(Bidirectional(SimpleRNN(8, recurrent_dropout=0.2, activation='relu')))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(16, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', \n",
    "                   patience=20, \n",
    "                   verbose=1,\n",
    "                   restore_best_weights=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5dc60a-d8b8-48d9-9ea9-fbed7c64dd74",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44d80056-cbf6-4888-94ff-95f25285b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b2654a2-69f1-4e79-b48a-cc51864d1d9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "90/90 [==============================] - 4s 11ms/step - loss: 0.4106 - accuracy: 0.7770 - val_loss: 0.3788 - val_accuracy: 0.7320\n",
      "Epoch 2/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.4124 - accuracy: 0.7826 - val_loss: 0.3904 - val_accuracy: 0.7062\n",
      "Epoch 3/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.4167 - accuracy: 0.7815 - val_loss: 0.3955 - val_accuracy: 0.7113\n",
      "Epoch 4/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3920 - accuracy: 0.7949 - val_loss: 0.3903 - val_accuracy: 0.7165\n",
      "Epoch 5/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3842 - accuracy: 0.8049 - val_loss: 0.3882 - val_accuracy: 0.7062\n",
      "Epoch 6/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3749 - accuracy: 0.7893 - val_loss: 0.3876 - val_accuracy: 0.7216\n",
      "Epoch 7/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3873 - accuracy: 0.7781 - val_loss: 0.3766 - val_accuracy: 0.7371\n",
      "Epoch 8/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3924 - accuracy: 0.7904 - val_loss: 0.3856 - val_accuracy: 0.7062\n",
      "Epoch 9/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3950 - accuracy: 0.7960 - val_loss: 0.3920 - val_accuracy: 0.7062\n",
      "Epoch 10/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3798 - accuracy: 0.7960 - val_loss: 0.3855 - val_accuracy: 0.7062\n",
      "Epoch 11/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3940 - accuracy: 0.7938 - val_loss: 0.3874 - val_accuracy: 0.7165\n",
      "Epoch 12/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3962 - accuracy: 0.7915 - val_loss: 0.3947 - val_accuracy: 0.6907\n",
      "Epoch 13/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3949 - accuracy: 0.7882 - val_loss: 0.3850 - val_accuracy: 0.7216\n",
      "Epoch 14/5000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.3886 - accuracy: 0.7904 - val_loss: 0.3921 - val_accuracy: 0.7062\n",
      "Epoch 15/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3733 - accuracy: 0.7971 - val_loss: 0.3884 - val_accuracy: 0.7165\n",
      "Epoch 16/5000\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.3772 - accuracy: 0.7938 - val_loss: 0.3872 - val_accuracy: 0.7165\n",
      "Epoch 17/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3696 - accuracy: 0.7971 - val_loss: 0.3777 - val_accuracy: 0.7371\n",
      "Epoch 18/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3650 - accuracy: 0.8049 - val_loss: 0.3832 - val_accuracy: 0.7062\n",
      "Epoch 19/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3824 - accuracy: 0.8004 - val_loss: 0.3901 - val_accuracy: 0.6959\n",
      "Epoch 20/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3749 - accuracy: 0.7949 - val_loss: 0.3824 - val_accuracy: 0.7320\n",
      "Epoch 21/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3745 - accuracy: 0.8038 - val_loss: 0.3817 - val_accuracy: 0.7320\n",
      "Epoch 22/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3796 - accuracy: 0.8038 - val_loss: 0.3868 - val_accuracy: 0.7165\n",
      "Epoch 23/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3709 - accuracy: 0.8004 - val_loss: 0.3869 - val_accuracy: 0.7268\n",
      "Epoch 24/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3690 - accuracy: 0.7982 - val_loss: 0.3840 - val_accuracy: 0.7320\n",
      "Epoch 25/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3755 - accuracy: 0.7915 - val_loss: 0.3824 - val_accuracy: 0.7320\n",
      "Epoch 26/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3770 - accuracy: 0.7960 - val_loss: 0.3843 - val_accuracy: 0.7165\n",
      "Epoch 27/5000\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3705 - accuracy: 0.8011Restoring model weights from the end of the best epoch: 7.\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3720 - accuracy: 0.7982 - val_loss: 0.3794 - val_accuracy: 0.7371\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4106 - accuracy: 0.7506 - val_loss: 0.3602 - val_accuracy: 0.8144\n",
      "Epoch 2/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.4022 - accuracy: 0.7494 - val_loss: 0.3614 - val_accuracy: 0.8093\n",
      "Epoch 3/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.4063 - accuracy: 0.7623 - val_loss: 0.3581 - val_accuracy: 0.8299\n",
      "Epoch 4/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3962 - accuracy: 0.7739 - val_loss: 0.3546 - val_accuracy: 0.8299\n",
      "Epoch 5/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3799 - accuracy: 0.7920 - val_loss: 0.3494 - val_accuracy: 0.8351\n",
      "Epoch 6/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3856 - accuracy: 0.7855 - val_loss: 0.3509 - val_accuracy: 0.8351\n",
      "Epoch 7/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3835 - accuracy: 0.7817 - val_loss: 0.3485 - val_accuracy: 0.8402\n",
      "Epoch 8/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3958 - accuracy: 0.7752 - val_loss: 0.3467 - val_accuracy: 0.8351\n",
      "Epoch 9/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3829 - accuracy: 0.7868 - val_loss: 0.3430 - val_accuracy: 0.8351\n",
      "Epoch 10/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3818 - accuracy: 0.7920 - val_loss: 0.3409 - val_accuracy: 0.8351\n",
      "Epoch 11/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3697 - accuracy: 0.7907 - val_loss: 0.3401 - val_accuracy: 0.8351\n",
      "Epoch 12/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3848 - accuracy: 0.7868 - val_loss: 0.3447 - val_accuracy: 0.8351\n",
      "Epoch 13/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3642 - accuracy: 0.7946 - val_loss: 0.3400 - val_accuracy: 0.8351\n",
      "Epoch 14/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3791 - accuracy: 0.7907 - val_loss: 0.3426 - val_accuracy: 0.8351\n",
      "Epoch 15/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3779 - accuracy: 0.7855 - val_loss: 0.3402 - val_accuracy: 0.8351\n",
      "Epoch 16/5000\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.3900 - accuracy: 0.7907 - val_loss: 0.3454 - val_accuracy: 0.8351\n",
      "Epoch 17/5000\n",
      "78/78 [==============================] - 1s 11ms/step - loss: 0.3763 - accuracy: 0.7868 - val_loss: 0.3393 - val_accuracy: 0.8351\n",
      "Epoch 18/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3659 - accuracy: 0.7959 - val_loss: 0.3391 - val_accuracy: 0.8351\n",
      "Epoch 19/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3906 - accuracy: 0.7946 - val_loss: 0.3384 - val_accuracy: 0.8351\n",
      "Epoch 20/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3710 - accuracy: 0.7907 - val_loss: 0.3392 - val_accuracy: 0.8351\n",
      "Epoch 21/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3707 - accuracy: 0.7868 - val_loss: 0.3360 - val_accuracy: 0.8351\n",
      "Epoch 22/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3681 - accuracy: 0.7907 - val_loss: 0.3407 - val_accuracy: 0.8351\n",
      "Epoch 23/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3935 - accuracy: 0.7804 - val_loss: 0.3411 - val_accuracy: 0.8351\n",
      "Epoch 24/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3692 - accuracy: 0.7984 - val_loss: 0.3365 - val_accuracy: 0.8351\n",
      "Epoch 25/5000\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.3674 - accuracy: 0.7907 - val_loss: 0.3415 - val_accuracy: 0.8351\n",
      "Epoch 26/5000\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3622 - accuracy: 0.7894 - val_loss: 0.3420 - val_accuracy: 0.8351\n",
      "Epoch 27/5000\n",
      "73/78 [===========================>..] - ETA: 0s - loss: 0.3534 - accuracy: 0.8055Restoring model weights from the end of the best epoch: 7.\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.3539 - accuracy: 0.8023 - val_loss: 0.3321 - val_accuracy: 0.8351\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4105 - accuracy: 0.7571 - val_loss: 0.3715 - val_accuracy: 0.7557\n",
      "Epoch 2/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.4089 - accuracy: 0.7657 - val_loss: 0.3717 - val_accuracy: 0.7614\n",
      "Epoch 3/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3909 - accuracy: 0.7857 - val_loss: 0.3732 - val_accuracy: 0.7557\n",
      "Epoch 4/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3934 - accuracy: 0.7643 - val_loss: 0.3794 - val_accuracy: 0.7500\n",
      "Epoch 5/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4012 - accuracy: 0.7614 - val_loss: 0.3727 - val_accuracy: 0.7670\n",
      "Epoch 6/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3967 - accuracy: 0.7700 - val_loss: 0.3828 - val_accuracy: 0.7500\n",
      "Epoch 7/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3838 - accuracy: 0.7857 - val_loss: 0.3811 - val_accuracy: 0.7670\n",
      "Epoch 8/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.4000 - accuracy: 0.7657 - val_loss: 0.3810 - val_accuracy: 0.7614\n",
      "Epoch 9/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3958 - accuracy: 0.7571 - val_loss: 0.3816 - val_accuracy: 0.7500\n",
      "Epoch 10/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3981 - accuracy: 0.7743 - val_loss: 0.3769 - val_accuracy: 0.7557\n",
      "Epoch 11/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3999 - accuracy: 0.7643 - val_loss: 0.3848 - val_accuracy: 0.7557\n",
      "Epoch 12/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3939 - accuracy: 0.7686 - val_loss: 0.3855 - val_accuracy: 0.7500\n",
      "Epoch 13/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3907 - accuracy: 0.7600 - val_loss: 0.3786 - val_accuracy: 0.7557\n",
      "Epoch 14/5000\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3978 - accuracy: 0.7714 - val_loss: 0.3767 - val_accuracy: 0.7500\n",
      "Epoch 15/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4000 - accuracy: 0.7486 - val_loss: 0.3826 - val_accuracy: 0.7557\n",
      "Epoch 16/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3850 - accuracy: 0.7771 - val_loss: 0.3754 - val_accuracy: 0.7727\n",
      "Epoch 17/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3870 - accuracy: 0.7857 - val_loss: 0.3858 - val_accuracy: 0.7670\n",
      "Epoch 18/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.4015 - accuracy: 0.7643 - val_loss: 0.3893 - val_accuracy: 0.7670\n",
      "Epoch 19/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3911 - accuracy: 0.7714 - val_loss: 0.3839 - val_accuracy: 0.7670\n",
      "Epoch 20/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - accuracy: 0.7914 - val_loss: 0.3858 - val_accuracy: 0.7670\n",
      "Epoch 21/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3792 - accuracy: 0.7700 - val_loss: 0.3843 - val_accuracy: 0.7670\n",
      "Epoch 22/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3800 - accuracy: 0.7657 - val_loss: 0.3865 - val_accuracy: 0.7557\n",
      "Epoch 23/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3738 - accuracy: 0.7886 - val_loss: 0.3882 - val_accuracy: 0.7386\n",
      "Epoch 24/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3859 - accuracy: 0.7643 - val_loss: 0.3773 - val_accuracy: 0.7614\n",
      "Epoch 25/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3765 - accuracy: 0.7843 - val_loss: 0.3766 - val_accuracy: 0.7670\n",
      "Epoch 26/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3876 - accuracy: 0.7614 - val_loss: 0.3858 - val_accuracy: 0.7614\n",
      "Epoch 27/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3917 - accuracy: 0.7871 - val_loss: 0.3894 - val_accuracy: 0.7443\n",
      "Epoch 28/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3960 - accuracy: 0.7786 - val_loss: 0.3897 - val_accuracy: 0.7557\n",
      "Epoch 29/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3804 - accuracy: 0.7600 - val_loss: 0.3881 - val_accuracy: 0.7443\n",
      "Epoch 30/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3780 - accuracy: 0.7643 - val_loss: 0.3828 - val_accuracy: 0.7614\n",
      "Epoch 31/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3810 - accuracy: 0.7729 - val_loss: 0.3828 - val_accuracy: 0.7557\n",
      "Epoch 32/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3987 - accuracy: 0.7643 - val_loss: 0.3918 - val_accuracy: 0.7557\n",
      "Epoch 33/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3796 - accuracy: 0.7857 - val_loss: 0.3939 - val_accuracy: 0.7614\n",
      "Epoch 34/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3723 - accuracy: 0.8000 - val_loss: 0.3889 - val_accuracy: 0.7727\n",
      "Epoch 35/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3821 - accuracy: 0.7800 - val_loss: 0.3856 - val_accuracy: 0.7557\n",
      "Epoch 36/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3800 - accuracy: 0.7671 - val_loss: 0.3877 - val_accuracy: 0.7784\n",
      "Epoch 37/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3825 - accuracy: 0.7729 - val_loss: 0.3939 - val_accuracy: 0.7557\n",
      "Epoch 38/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3762 - accuracy: 0.7743 - val_loss: 0.3918 - val_accuracy: 0.7784\n",
      "Epoch 39/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3858 - accuracy: 0.7771 - val_loss: 0.3868 - val_accuracy: 0.7614\n",
      "Epoch 40/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3820 - accuracy: 0.7714 - val_loss: 0.3879 - val_accuracy: 0.7670\n",
      "Epoch 41/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3827 - accuracy: 0.7829 - val_loss: 0.3862 - val_accuracy: 0.7955\n",
      "Epoch 42/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3795 - accuracy: 0.7857 - val_loss: 0.3857 - val_accuracy: 0.7898\n",
      "Epoch 43/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3764 - accuracy: 0.7857 - val_loss: 0.3885 - val_accuracy: 0.7898\n",
      "Epoch 44/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3782 - accuracy: 0.7814 - val_loss: 0.3878 - val_accuracy: 0.7614\n",
      "Epoch 45/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3705 - accuracy: 0.7700 - val_loss: 0.3910 - val_accuracy: 0.7614\n",
      "Epoch 46/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3806 - accuracy: 0.7800 - val_loss: 0.3906 - val_accuracy: 0.7614\n",
      "Epoch 47/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3798 - accuracy: 0.7686 - val_loss: 0.3907 - val_accuracy: 0.7614\n",
      "Epoch 48/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3697 - accuracy: 0.7771 - val_loss: 0.3911 - val_accuracy: 0.7557\n",
      "Epoch 49/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3756 - accuracy: 0.7700 - val_loss: 0.3928 - val_accuracy: 0.7557\n",
      "Epoch 50/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3615 - accuracy: 0.7914 - val_loss: 0.3930 - val_accuracy: 0.7557\n",
      "Epoch 51/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3721 - accuracy: 0.7743 - val_loss: 0.3938 - val_accuracy: 0.7557\n",
      "Epoch 52/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3745 - accuracy: 0.7786 - val_loss: 0.3927 - val_accuracy: 0.7670\n",
      "Epoch 53/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3710 - accuracy: 0.7857 - val_loss: 0.3963 - val_accuracy: 0.7841\n",
      "Epoch 54/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3755 - accuracy: 0.7771 - val_loss: 0.3920 - val_accuracy: 0.7614\n",
      "Epoch 55/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3783 - accuracy: 0.7929 - val_loss: 0.3935 - val_accuracy: 0.7557\n",
      "Epoch 56/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3638 - accuracy: 0.7957 - val_loss: 0.4027 - val_accuracy: 0.7727\n",
      "Epoch 57/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3671 - accuracy: 0.7900 - val_loss: 0.3903 - val_accuracy: 0.7670\n",
      "Epoch 58/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3622 - accuracy: 0.7900 - val_loss: 0.3918 - val_accuracy: 0.7557\n",
      "Epoch 59/5000\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 0.3683 - accuracy: 0.7943 - val_loss: 0.3963 - val_accuracy: 0.7670\n",
      "Epoch 60/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3746 - accuracy: 0.7857 - val_loss: 0.3908 - val_accuracy: 0.7898\n",
      "Epoch 61/5000\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3718 - accuracy: 0.7692Restoring model weights from the end of the best epoch: 41.\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3702 - accuracy: 0.7743 - val_loss: 0.3959 - val_accuracy: 0.7784\n",
      "Epoch 61: early stopping\n",
      "Epoch 1/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3910 - accuracy: 0.7492 - val_loss: 0.3747 - val_accuracy: 0.7744\n",
      "Epoch 2/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3758 - accuracy: 0.7569 - val_loss: 0.3584 - val_accuracy: 0.7866\n",
      "Epoch 3/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3818 - accuracy: 0.7508 - val_loss: 0.3691 - val_accuracy: 0.7988\n",
      "Epoch 4/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3713 - accuracy: 0.7768 - val_loss: 0.3577 - val_accuracy: 0.8171\n",
      "Epoch 5/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3703 - accuracy: 0.7829 - val_loss: 0.3626 - val_accuracy: 0.8049\n",
      "Epoch 6/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3724 - accuracy: 0.7706 - val_loss: 0.3589 - val_accuracy: 0.8049\n",
      "Epoch 7/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3825 - accuracy: 0.7768 - val_loss: 0.3602 - val_accuracy: 0.8293\n",
      "Epoch 8/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3771 - accuracy: 0.7737 - val_loss: 0.3539 - val_accuracy: 0.8293\n",
      "Epoch 9/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3726 - accuracy: 0.7722 - val_loss: 0.3528 - val_accuracy: 0.8293\n",
      "Epoch 10/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3718 - accuracy: 0.7783 - val_loss: 0.3595 - val_accuracy: 0.8171\n",
      "Epoch 11/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3684 - accuracy: 0.7706 - val_loss: 0.3516 - val_accuracy: 0.8232\n",
      "Epoch 12/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3633 - accuracy: 0.7722 - val_loss: 0.3591 - val_accuracy: 0.8110\n",
      "Epoch 13/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3696 - accuracy: 0.7706 - val_loss: 0.3609 - val_accuracy: 0.8049\n",
      "Epoch 14/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3802 - accuracy: 0.7645 - val_loss: 0.3627 - val_accuracy: 0.8110\n",
      "Epoch 15/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3623 - accuracy: 0.7966 - val_loss: 0.3528 - val_accuracy: 0.8110\n",
      "Epoch 16/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3584 - accuracy: 0.7905 - val_loss: 0.3530 - val_accuracy: 0.8171\n",
      "Epoch 17/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3725 - accuracy: 0.7844 - val_loss: 0.3600 - val_accuracy: 0.8110\n",
      "Epoch 18/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3609 - accuracy: 0.7737 - val_loss: 0.3541 - val_accuracy: 0.8110\n",
      "Epoch 19/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3604 - accuracy: 0.7768 - val_loss: 0.3543 - val_accuracy: 0.8171\n",
      "Epoch 20/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3697 - accuracy: 0.7615 - val_loss: 0.3546 - val_accuracy: 0.8110\n",
      "Epoch 21/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3738 - accuracy: 0.7798 - val_loss: 0.3579 - val_accuracy: 0.8049\n",
      "Epoch 22/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3623 - accuracy: 0.7875 - val_loss: 0.3530 - val_accuracy: 0.8110\n",
      "Epoch 23/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3788 - accuracy: 0.7599 - val_loss: 0.3633 - val_accuracy: 0.8232\n",
      "Epoch 24/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3606 - accuracy: 0.7783 - val_loss: 0.3601 - val_accuracy: 0.8110\n",
      "Epoch 25/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3616 - accuracy: 0.7722 - val_loss: 0.3567 - val_accuracy: 0.8171\n",
      "Epoch 26/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3691 - accuracy: 0.7783 - val_loss: 0.3589 - val_accuracy: 0.8110\n",
      "Epoch 27/5000\n",
      "61/66 [==========================>...] - ETA: 0s - loss: 0.3716 - accuracy: 0.7803Restoring model weights from the end of the best epoch: 7.\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3700 - accuracy: 0.7813 - val_loss: 0.3587 - val_accuracy: 0.8171\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3906 - accuracy: 0.7590 - val_loss: 0.3544 - val_accuracy: 0.7387\n",
      "Epoch 2/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3963 - accuracy: 0.7782 - val_loss: 0.3563 - val_accuracy: 0.7477\n",
      "Epoch 3/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3893 - accuracy: 0.7658 - val_loss: 0.3545 - val_accuracy: 0.7523\n",
      "Epoch 4/5000\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.3740 - accuracy: 0.7770 - val_loss: 0.3529 - val_accuracy: 0.7793\n",
      "Epoch 5/5000\n",
      "89/89 [==============================] - 1s 11ms/step - loss: 0.3892 - accuracy: 0.7725 - val_loss: 0.3538 - val_accuracy: 0.7568\n",
      "Epoch 6/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3686 - accuracy: 0.7703 - val_loss: 0.3545 - val_accuracy: 0.7342\n",
      "Epoch 7/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3812 - accuracy: 0.7703 - val_loss: 0.3541 - val_accuracy: 0.7477\n",
      "Epoch 8/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3742 - accuracy: 0.7725 - val_loss: 0.3560 - val_accuracy: 0.7342\n",
      "Epoch 9/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3739 - accuracy: 0.7770 - val_loss: 0.3542 - val_accuracy: 0.7568\n",
      "Epoch 10/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3680 - accuracy: 0.7782 - val_loss: 0.3541 - val_accuracy: 0.7613\n",
      "Epoch 11/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3730 - accuracy: 0.7725 - val_loss: 0.3562 - val_accuracy: 0.7523\n",
      "Epoch 12/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3859 - accuracy: 0.7782 - val_loss: 0.3568 - val_accuracy: 0.7432\n",
      "Epoch 13/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3729 - accuracy: 0.7736 - val_loss: 0.3535 - val_accuracy: 0.7568\n",
      "Epoch 14/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3739 - accuracy: 0.7748 - val_loss: 0.3555 - val_accuracy: 0.7387\n",
      "Epoch 15/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3671 - accuracy: 0.7748 - val_loss: 0.3547 - val_accuracy: 0.7342\n",
      "Epoch 16/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3700 - accuracy: 0.7669 - val_loss: 0.3593 - val_accuracy: 0.7342\n",
      "Epoch 17/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3760 - accuracy: 0.7624 - val_loss: 0.3560 - val_accuracy: 0.7387\n",
      "Epoch 18/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3793 - accuracy: 0.7691 - val_loss: 0.3558 - val_accuracy: 0.7162\n",
      "Epoch 19/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3863 - accuracy: 0.7714 - val_loss: 0.3570 - val_accuracy: 0.7252\n",
      "Epoch 20/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3699 - accuracy: 0.7804 - val_loss: 0.3554 - val_accuracy: 0.7252\n",
      "Epoch 21/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3835 - accuracy: 0.7691 - val_loss: 0.3566 - val_accuracy: 0.7342\n",
      "Epoch 22/5000\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3783 - accuracy: 0.7782 - val_loss: 0.3579 - val_accuracy: 0.7387\n",
      "Epoch 23/5000\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 0.3671 - accuracy: 0.7714 - val_loss: 0.3571 - val_accuracy: 0.7432\n",
      "Epoch 24/5000\n",
      "85/89 [===========================>..] - ETA: 0s - loss: 0.3845 - accuracy: 0.7624Restoring model weights from the end of the best epoch: 4.\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 0.3850 - accuracy: 0.7624 - val_loss: 0.3556 - val_accuracy: 0.7297\n",
      "Epoch 24: early stopping\n",
      "Epoch 1/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3773 - accuracy: 0.8073 - val_loss: 0.3844 - val_accuracy: 0.6909\n",
      "Epoch 2/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3749 - accuracy: 0.8027 - val_loss: 0.3880 - val_accuracy: 0.7030\n",
      "Epoch 3/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3814 - accuracy: 0.8149 - val_loss: 0.3976 - val_accuracy: 0.6788\n",
      "Epoch 4/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3623 - accuracy: 0.8149 - val_loss: 0.3917 - val_accuracy: 0.7030\n",
      "Epoch 5/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3716 - accuracy: 0.8194 - val_loss: 0.3943 - val_accuracy: 0.6848\n",
      "Epoch 6/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3563 - accuracy: 0.8209 - val_loss: 0.3865 - val_accuracy: 0.6970\n",
      "Epoch 7/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3613 - accuracy: 0.8134 - val_loss: 0.4121 - val_accuracy: 0.7030\n",
      "Epoch 8/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3632 - accuracy: 0.8134 - val_loss: 0.4072 - val_accuracy: 0.7030\n",
      "Epoch 9/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3723 - accuracy: 0.8118 - val_loss: 0.4014 - val_accuracy: 0.6970\n",
      "Epoch 10/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3659 - accuracy: 0.8209 - val_loss: 0.4017 - val_accuracy: 0.6848\n",
      "Epoch 11/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3550 - accuracy: 0.8240 - val_loss: 0.4108 - val_accuracy: 0.6909\n",
      "Epoch 12/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3695 - accuracy: 0.8149 - val_loss: 0.4138 - val_accuracy: 0.6788\n",
      "Epoch 13/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3644 - accuracy: 0.8164 - val_loss: 0.4039 - val_accuracy: 0.6909\n",
      "Epoch 14/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3561 - accuracy: 0.8118 - val_loss: 0.4204 - val_accuracy: 0.6545\n",
      "Epoch 15/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3699 - accuracy: 0.8103 - val_loss: 0.4137 - val_accuracy: 0.6606\n",
      "Epoch 16/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3541 - accuracy: 0.8149 - val_loss: 0.4176 - val_accuracy: 0.6545\n",
      "Epoch 17/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3506 - accuracy: 0.8149 - val_loss: 0.4130 - val_accuracy: 0.6788\n",
      "Epoch 18/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3527 - accuracy: 0.8164 - val_loss: 0.4007 - val_accuracy: 0.6848\n",
      "Epoch 19/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3527 - accuracy: 0.8179 - val_loss: 0.3982 - val_accuracy: 0.6848\n",
      "Epoch 20/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3601 - accuracy: 0.8118 - val_loss: 0.4025 - val_accuracy: 0.7091\n",
      "Epoch 21/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3617 - accuracy: 0.8103 - val_loss: 0.4048 - val_accuracy: 0.6848\n",
      "Epoch 22/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3567 - accuracy: 0.8164 - val_loss: 0.3985 - val_accuracy: 0.6909\n",
      "Epoch 23/5000\n",
      "66/66 [==============================] - 1s 13ms/step - loss: 0.3548 - accuracy: 0.8164 - val_loss: 0.4058 - val_accuracy: 0.6788\n",
      "Epoch 24/5000\n",
      "66/66 [==============================] - 1s 13ms/step - loss: 0.3494 - accuracy: 0.8149 - val_loss: 0.4056 - val_accuracy: 0.6788\n",
      "Epoch 25/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3491 - accuracy: 0.8179 - val_loss: 0.3986 - val_accuracy: 0.6788\n",
      "Epoch 26/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3542 - accuracy: 0.8164 - val_loss: 0.4109 - val_accuracy: 0.6667\n",
      "Epoch 27/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3562 - accuracy: 0.8118 - val_loss: 0.4079 - val_accuracy: 0.6667\n",
      "Epoch 28/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3486 - accuracy: 0.8164 - val_loss: 0.4027 - val_accuracy: 0.6727\n",
      "Epoch 29/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3635 - accuracy: 0.8118 - val_loss: 0.4004 - val_accuracy: 0.7152\n",
      "Epoch 30/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3487 - accuracy: 0.8209 - val_loss: 0.4046 - val_accuracy: 0.6788\n",
      "Epoch 31/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3503 - accuracy: 0.8149 - val_loss: 0.4019 - val_accuracy: 0.6848\n",
      "Epoch 32/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3563 - accuracy: 0.8103 - val_loss: 0.3997 - val_accuracy: 0.6909\n",
      "Epoch 33/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3423 - accuracy: 0.8179 - val_loss: 0.4009 - val_accuracy: 0.6788\n",
      "Epoch 34/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3494 - accuracy: 0.8194 - val_loss: 0.4108 - val_accuracy: 0.6909\n",
      "Epoch 35/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3417 - accuracy: 0.8194 - val_loss: 0.4068 - val_accuracy: 0.6727\n",
      "Epoch 36/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3577 - accuracy: 0.8164 - val_loss: 0.4199 - val_accuracy: 0.6545\n",
      "Epoch 37/5000\n",
      "66/66 [==============================] - 1s 15ms/step - loss: 0.3469 - accuracy: 0.8149 - val_loss: 0.4185 - val_accuracy: 0.6667\n",
      "Epoch 38/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3454 - accuracy: 0.8149 - val_loss: 0.4090 - val_accuracy: 0.6848\n",
      "Epoch 39/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3380 - accuracy: 0.8225 - val_loss: 0.4002 - val_accuracy: 0.7030\n",
      "Epoch 40/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3481 - accuracy: 0.8073 - val_loss: 0.4173 - val_accuracy: 0.6606\n",
      "Epoch 41/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3409 - accuracy: 0.8179 - val_loss: 0.4174 - val_accuracy: 0.6606\n",
      "Epoch 42/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3429 - accuracy: 0.8134 - val_loss: 0.4017 - val_accuracy: 0.6848\n",
      "Epoch 43/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3488 - accuracy: 0.8164 - val_loss: 0.3996 - val_accuracy: 0.6788\n",
      "Epoch 44/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3445 - accuracy: 0.8240 - val_loss: 0.3994 - val_accuracy: 0.6727\n",
      "Epoch 45/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3331 - accuracy: 0.8240 - val_loss: 0.4025 - val_accuracy: 0.6788\n",
      "Epoch 46/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3481 - accuracy: 0.8209 - val_loss: 0.4133 - val_accuracy: 0.6667\n",
      "Epoch 47/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3499 - accuracy: 0.8149 - val_loss: 0.4149 - val_accuracy: 0.6667\n",
      "Epoch 48/5000\n",
      "66/66 [==============================] - 1s 13ms/step - loss: 0.3370 - accuracy: 0.8179 - val_loss: 0.4014 - val_accuracy: 0.6848\n",
      "Epoch 49/5000\n",
      "66/66 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.8179Restoring model weights from the end of the best epoch: 29.\n",
      "66/66 [==============================] - 1s 18ms/step - loss: 0.3383 - accuracy: 0.8179 - val_loss: 0.4024 - val_accuracy: 0.6848\n",
      "Epoch 49: early stopping\n",
      "Epoch 1/5000\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.3935 - accuracy: 0.7390 - val_loss: 0.3589 - val_accuracy: 0.7500\n",
      "Epoch 2/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3767 - accuracy: 0.7412 - val_loss: 0.3570 - val_accuracy: 0.7368\n",
      "Epoch 3/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3707 - accuracy: 0.7533 - val_loss: 0.3550 - val_accuracy: 0.7500\n",
      "Epoch 4/5000\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.3739 - accuracy: 0.7522 - val_loss: 0.3578 - val_accuracy: 0.7368\n",
      "Epoch 5/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3793 - accuracy: 0.7379 - val_loss: 0.3548 - val_accuracy: 0.7412\n",
      "Epoch 6/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3657 - accuracy: 0.7368 - val_loss: 0.3547 - val_accuracy: 0.7588\n",
      "Epoch 7/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3717 - accuracy: 0.7511 - val_loss: 0.3537 - val_accuracy: 0.7675\n",
      "Epoch 8/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3785 - accuracy: 0.7522 - val_loss: 0.3531 - val_accuracy: 0.7412\n",
      "Epoch 9/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3705 - accuracy: 0.7555 - val_loss: 0.3525 - val_accuracy: 0.7412\n",
      "Epoch 10/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3697 - accuracy: 0.7522 - val_loss: 0.3530 - val_accuracy: 0.7412\n",
      "Epoch 11/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3683 - accuracy: 0.7588 - val_loss: 0.3564 - val_accuracy: 0.7368\n",
      "Epoch 12/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3743 - accuracy: 0.7522 - val_loss: 0.3553 - val_accuracy: 0.7588\n",
      "Epoch 13/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3681 - accuracy: 0.7478 - val_loss: 0.3528 - val_accuracy: 0.7544\n",
      "Epoch 14/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3710 - accuracy: 0.7621 - val_loss: 0.3566 - val_accuracy: 0.7588\n",
      "Epoch 15/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3689 - accuracy: 0.7544 - val_loss: 0.3633 - val_accuracy: 0.7281\n",
      "Epoch 16/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3923 - accuracy: 0.7335 - val_loss: 0.3558 - val_accuracy: 0.7544\n",
      "Epoch 17/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3758 - accuracy: 0.7522 - val_loss: 0.3603 - val_accuracy: 0.7500\n",
      "Epoch 18/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3603 - accuracy: 0.7731 - val_loss: 0.3550 - val_accuracy: 0.7500\n",
      "Epoch 19/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3621 - accuracy: 0.7588 - val_loss: 0.3579 - val_accuracy: 0.7368\n",
      "Epoch 20/5000\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.3629 - accuracy: 0.7632 - val_loss: 0.3541 - val_accuracy: 0.7544\n",
      "Epoch 21/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3756 - accuracy: 0.7665 - val_loss: 0.3563 - val_accuracy: 0.7368\n",
      "Epoch 22/5000\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3805 - accuracy: 0.7511 - val_loss: 0.3532 - val_accuracy: 0.7675\n",
      "Epoch 23/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3712 - accuracy: 0.7533 - val_loss: 0.3554 - val_accuracy: 0.7588\n",
      "Epoch 24/5000\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.3642 - accuracy: 0.7621 - val_loss: 0.3565 - val_accuracy: 0.7368\n",
      "Epoch 25/5000\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.3699 - accuracy: 0.7588 - val_loss: 0.3573 - val_accuracy: 0.7456\n",
      "Epoch 26/5000\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.3610 - accuracy: 0.7522 - val_loss: 0.3567 - val_accuracy: 0.7368\n",
      "Epoch 27/5000\n",
      "87/91 [===========================>..] - ETA: 0s - loss: 0.3585 - accuracy: 0.7759Restoring model weights from the end of the best epoch: 7.\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.3613 - accuracy: 0.7720 - val_loss: 0.3574 - val_accuracy: 0.7412\n",
      "Epoch 27: early stopping\n",
      "Epoch 1/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3701 - accuracy: 0.7811 - val_loss: 0.3387 - val_accuracy: 0.8272\n",
      "Epoch 2/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3751 - accuracy: 0.7955 - val_loss: 0.3472 - val_accuracy: 0.8272\n",
      "Epoch 3/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3833 - accuracy: 0.7982 - val_loss: 0.3313 - val_accuracy: 0.8272\n",
      "Epoch 4/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3876 - accuracy: 0.7969 - val_loss: 0.3343 - val_accuracy: 0.8272\n",
      "Epoch 5/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3696 - accuracy: 0.8008 - val_loss: 0.3296 - val_accuracy: 0.8272\n",
      "Epoch 6/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3649 - accuracy: 0.8021 - val_loss: 0.3311 - val_accuracy: 0.8272\n",
      "Epoch 7/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3593 - accuracy: 0.8073 - val_loss: 0.3311 - val_accuracy: 0.8220\n",
      "Epoch 8/5000\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3586 - accuracy: 0.7995 - val_loss: 0.3351 - val_accuracy: 0.8272\n",
      "Epoch 9/5000\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.3589 - accuracy: 0.7969 - val_loss: 0.3292 - val_accuracy: 0.8272\n",
      "Epoch 10/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3738 - accuracy: 0.7916 - val_loss: 0.3298 - val_accuracy: 0.8272\n",
      "Epoch 11/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3653 - accuracy: 0.7982 - val_loss: 0.3332 - val_accuracy: 0.8272\n",
      "Epoch 12/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3661 - accuracy: 0.7982 - val_loss: 0.3283 - val_accuracy: 0.8272\n",
      "Epoch 13/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3608 - accuracy: 0.7969 - val_loss: 0.3313 - val_accuracy: 0.8272\n",
      "Epoch 14/5000\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.3596 - accuracy: 0.7995 - val_loss: 0.3278 - val_accuracy: 0.8272\n",
      "Epoch 15/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3674 - accuracy: 0.7969 - val_loss: 0.3335 - val_accuracy: 0.8272\n",
      "Epoch 16/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3476 - accuracy: 0.8034 - val_loss: 0.3241 - val_accuracy: 0.8272\n",
      "Epoch 17/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3523 - accuracy: 0.8021 - val_loss: 0.3248 - val_accuracy: 0.8272\n",
      "Epoch 18/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3594 - accuracy: 0.7942 - val_loss: 0.3267 - val_accuracy: 0.8272\n",
      "Epoch 19/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3518 - accuracy: 0.8073 - val_loss: 0.3271 - val_accuracy: 0.8272\n",
      "Epoch 20/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3570 - accuracy: 0.7982 - val_loss: 0.3269 - val_accuracy: 0.8272\n",
      "Epoch 21/5000\n",
      "76/77 [============================>.] - ETA: 0s - loss: 0.3483 - accuracy: 0.8053Restoring model weights from the end of the best epoch: 1.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3486 - accuracy: 0.8047 - val_loss: 0.3331 - val_accuracy: 0.8272\n",
      "Epoch 21: early stopping\n",
      "Epoch 1/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3854 - accuracy: 0.7275 - val_loss: 0.3360 - val_accuracy: 0.8909\n",
      "Epoch 2/5000\n",
      "66/66 [==============================] - 1s 14ms/step - loss: 0.3728 - accuracy: 0.8067 - val_loss: 0.3184 - val_accuracy: 0.9030\n",
      "Epoch 3/5000\n",
      "66/66 [==============================] - 1s 16ms/step - loss: 0.3600 - accuracy: 0.8158 - val_loss: 0.3126 - val_accuracy: 0.9030\n",
      "Epoch 4/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3551 - accuracy: 0.8128 - val_loss: 0.2946 - val_accuracy: 0.9030\n",
      "Epoch 5/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3582 - accuracy: 0.8128 - val_loss: 0.2905 - val_accuracy: 0.9030\n",
      "Epoch 6/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3524 - accuracy: 0.8219 - val_loss: 0.2961 - val_accuracy: 0.9030\n",
      "Epoch 7/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3614 - accuracy: 0.8143 - val_loss: 0.2910 - val_accuracy: 0.9030\n",
      "Epoch 8/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3488 - accuracy: 0.8189 - val_loss: 0.2959 - val_accuracy: 0.9030\n",
      "Epoch 9/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3516 - accuracy: 0.8158 - val_loss: 0.2832 - val_accuracy: 0.9030\n",
      "Epoch 10/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3720 - accuracy: 0.8158 - val_loss: 0.2928 - val_accuracy: 0.9030\n",
      "Epoch 11/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3514 - accuracy: 0.8128 - val_loss: 0.2868 - val_accuracy: 0.9030\n",
      "Epoch 12/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3749 - accuracy: 0.8189 - val_loss: 0.2884 - val_accuracy: 0.9030\n",
      "Epoch 13/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3473 - accuracy: 0.8204 - val_loss: 0.2912 - val_accuracy: 0.9030\n",
      "Epoch 14/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3450 - accuracy: 0.8158 - val_loss: 0.2818 - val_accuracy: 0.9030\n",
      "Epoch 15/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3608 - accuracy: 0.8128 - val_loss: 0.2885 - val_accuracy: 0.9030\n",
      "Epoch 16/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3502 - accuracy: 0.8174 - val_loss: 0.2806 - val_accuracy: 0.9030\n",
      "Epoch 17/5000\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.3409 - accuracy: 0.8219 - val_loss: 0.2837 - val_accuracy: 0.9030\n",
      "Epoch 18/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3533 - accuracy: 0.8128 - val_loss: 0.2818 - val_accuracy: 0.9030\n",
      "Epoch 19/5000\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.3436 - accuracy: 0.8158 - val_loss: 0.2848 - val_accuracy: 0.9030\n",
      "Epoch 20/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3483 - accuracy: 0.8189 - val_loss: 0.2835 - val_accuracy: 0.9030\n",
      "Epoch 21/5000\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3488 - accuracy: 0.8158 - val_loss: 0.2805 - val_accuracy: 0.9030\n",
      "Epoch 22/5000\n",
      "61/66 [==========================>...] - ETA: 0s - loss: 0.3445 - accuracy: 0.8131Restoring model weights from the end of the best epoch: 2.\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.3435 - accuracy: 0.8143 - val_loss: 0.2821 - val_accuracy: 0.9030\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3964 - accuracy: 0.7126 - val_loss: 0.3583 - val_accuracy: 0.7713\n",
      "Epoch 2/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3925 - accuracy: 0.7701 - val_loss: 0.3520 - val_accuracy: 0.7447\n",
      "Epoch 3/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3823 - accuracy: 0.7741 - val_loss: 0.3493 - val_accuracy: 0.7660\n",
      "Epoch 4/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3773 - accuracy: 0.7634 - val_loss: 0.3490 - val_accuracy: 0.7766\n",
      "Epoch 5/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3751 - accuracy: 0.7634 - val_loss: 0.3491 - val_accuracy: 0.7819\n",
      "Epoch 6/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3805 - accuracy: 0.7727 - val_loss: 0.3510 - val_accuracy: 0.7819\n",
      "Epoch 7/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3779 - accuracy: 0.7741 - val_loss: 0.3493 - val_accuracy: 0.7872\n",
      "Epoch 8/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3841 - accuracy: 0.7741 - val_loss: 0.3541 - val_accuracy: 0.7766\n",
      "Epoch 9/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3789 - accuracy: 0.7701 - val_loss: 0.3508 - val_accuracy: 0.7766\n",
      "Epoch 10/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3659 - accuracy: 0.7821 - val_loss: 0.3507 - val_accuracy: 0.7979\n",
      "Epoch 11/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3695 - accuracy: 0.7767 - val_loss: 0.3517 - val_accuracy: 0.7926\n",
      "Epoch 12/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3664 - accuracy: 0.7794 - val_loss: 0.3516 - val_accuracy: 0.7766\n",
      "Epoch 13/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3636 - accuracy: 0.7754 - val_loss: 0.3507 - val_accuracy: 0.7979\n",
      "Epoch 14/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3641 - accuracy: 0.7821 - val_loss: 0.3527 - val_accuracy: 0.7979\n",
      "Epoch 15/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3680 - accuracy: 0.7781 - val_loss: 0.3513 - val_accuracy: 0.7872\n",
      "Epoch 16/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3590 - accuracy: 0.7955 - val_loss: 0.3505 - val_accuracy: 0.7926\n",
      "Epoch 17/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3731 - accuracy: 0.7727 - val_loss: 0.3535 - val_accuracy: 0.7926\n",
      "Epoch 18/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3617 - accuracy: 0.7781 - val_loss: 0.3513 - val_accuracy: 0.7926\n",
      "Epoch 19/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3637 - accuracy: 0.7714 - val_loss: 0.3511 - val_accuracy: 0.7819\n",
      "Epoch 20/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3663 - accuracy: 0.7848 - val_loss: 0.3520 - val_accuracy: 0.7606\n",
      "Epoch 21/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3681 - accuracy: 0.7741 - val_loss: 0.3514 - val_accuracy: 0.7713\n",
      "Epoch 22/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3585 - accuracy: 0.7955 - val_loss: 0.3503 - val_accuracy: 0.7660\n",
      "Epoch 23/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3608 - accuracy: 0.7928 - val_loss: 0.3504 - val_accuracy: 0.7713\n",
      "Epoch 24/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3666 - accuracy: 0.7874 - val_loss: 0.3526 - val_accuracy: 0.7819\n",
      "Epoch 25/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3640 - accuracy: 0.7861 - val_loss: 0.3516 - val_accuracy: 0.7819\n",
      "Epoch 26/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3608 - accuracy: 0.7955 - val_loss: 0.3548 - val_accuracy: 0.7819\n",
      "Epoch 27/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3563 - accuracy: 0.7981 - val_loss: 0.3546 - val_accuracy: 0.7766\n",
      "Epoch 28/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3728 - accuracy: 0.7794 - val_loss: 0.3523 - val_accuracy: 0.7926\n",
      "Epoch 29/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3730 - accuracy: 0.7781 - val_loss: 0.3519 - val_accuracy: 0.7713\n",
      "Epoch 30/5000\n",
      "70/75 [===========================>..] - ETA: 0s - loss: 0.3594 - accuracy: 0.7886Restoring model weights from the end of the best epoch: 10.\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3590 - accuracy: 0.7901 - val_loss: 0.3537 - val_accuracy: 0.7500\n",
      "Epoch 30: early stopping\n",
      "Epoch 1/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3835 - accuracy: 0.7762 - val_loss: 0.3461 - val_accuracy: 0.8114\n",
      "Epoch 2/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3790 - accuracy: 0.7934 - val_loss: 0.3205 - val_accuracy: 0.8343\n",
      "Epoch 3/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3630 - accuracy: 0.7963 - val_loss: 0.3149 - val_accuracy: 0.8343\n",
      "Epoch 4/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3868 - accuracy: 0.7977 - val_loss: 0.3315 - val_accuracy: 0.8343\n",
      "Epoch 5/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3747 - accuracy: 0.8006 - val_loss: 0.3145 - val_accuracy: 0.8400\n",
      "Epoch 6/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3791 - accuracy: 0.7963 - val_loss: 0.3169 - val_accuracy: 0.8286\n",
      "Epoch 7/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3847 - accuracy: 0.7963 - val_loss: 0.3188 - val_accuracy: 0.8343\n",
      "Epoch 8/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3639 - accuracy: 0.7877 - val_loss: 0.3195 - val_accuracy: 0.8343\n",
      "Epoch 9/5000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3644 - accuracy: 0.7977 - val_loss: 0.3156 - val_accuracy: 0.8343\n",
      "Epoch 10/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3573 - accuracy: 0.8034 - val_loss: 0.3169 - val_accuracy: 0.8229\n",
      "Epoch 11/5000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3896 - accuracy: 0.7877 - val_loss: 0.3144 - val_accuracy: 0.8229\n",
      "Epoch 12/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3733 - accuracy: 0.7862 - val_loss: 0.3174 - val_accuracy: 0.8343\n",
      "Epoch 13/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3627 - accuracy: 0.8006 - val_loss: 0.3176 - val_accuracy: 0.8286\n",
      "Epoch 14/5000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3626 - accuracy: 0.8106 - val_loss: 0.3170 - val_accuracy: 0.8286\n",
      "Epoch 15/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3584 - accuracy: 0.7963 - val_loss: 0.3108 - val_accuracy: 0.8343\n",
      "Epoch 16/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3649 - accuracy: 0.8077 - val_loss: 0.3138 - val_accuracy: 0.8343\n",
      "Epoch 17/5000\n",
      "70/70 [==============================] - 1s 11ms/step - loss: 0.3591 - accuracy: 0.8049 - val_loss: 0.3196 - val_accuracy: 0.8400\n",
      "Epoch 18/5000\n",
      "70/70 [==============================] - 1s 17ms/step - loss: 0.3680 - accuracy: 0.7920 - val_loss: 0.3191 - val_accuracy: 0.8343\n",
      "Epoch 19/5000\n",
      "70/70 [==============================] - 1s 12ms/step - loss: 0.3596 - accuracy: 0.7791 - val_loss: 0.3173 - val_accuracy: 0.8400\n",
      "Epoch 20/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3667 - accuracy: 0.8006 - val_loss: 0.3161 - val_accuracy: 0.8171\n",
      "Epoch 21/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3612 - accuracy: 0.8034 - val_loss: 0.3145 - val_accuracy: 0.8229\n",
      "Epoch 22/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3554 - accuracy: 0.8034 - val_loss: 0.3143 - val_accuracy: 0.8343\n",
      "Epoch 23/5000\n",
      "70/70 [==============================] - 1s 9ms/step - loss: 0.3667 - accuracy: 0.8020 - val_loss: 0.3143 - val_accuracy: 0.8400\n",
      "Epoch 24/5000\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3605 - accuracy: 0.8077 - val_loss: 0.3139 - val_accuracy: 0.8286\n",
      "Epoch 25/5000\n",
      "65/70 [==========================>...] - ETA: 0s - loss: 0.3547 - accuracy: 0.7938Restoring model weights from the end of the best epoch: 5.\n",
      "70/70 [==============================] - 1s 10ms/step - loss: 0.3494 - accuracy: 0.8006 - val_loss: 0.3099 - val_accuracy: 0.8400\n",
      "Epoch 25: early stopping\n",
      "Epoch 1/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3787 - accuracy: 0.7673 - val_loss: 0.3554 - val_accuracy: 0.7798\n",
      "Epoch 2/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3783 - accuracy: 0.7673 - val_loss: 0.3535 - val_accuracy: 0.7798\n",
      "Epoch 3/5000\n",
      "87/87 [==============================] - 1s 13ms/step - loss: 0.3764 - accuracy: 0.7661 - val_loss: 0.3531 - val_accuracy: 0.7890\n",
      "Epoch 4/5000\n",
      "87/87 [==============================] - 1s 15ms/step - loss: 0.3721 - accuracy: 0.7661 - val_loss: 0.3486 - val_accuracy: 0.7936\n",
      "Epoch 5/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3708 - accuracy: 0.7707 - val_loss: 0.3522 - val_accuracy: 0.7936\n",
      "Epoch 6/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3762 - accuracy: 0.7753 - val_loss: 0.3501 - val_accuracy: 0.7982\n",
      "Epoch 7/5000\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3796 - accuracy: 0.7661 - val_loss: 0.3488 - val_accuracy: 0.7890\n",
      "Epoch 8/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3744 - accuracy: 0.7742 - val_loss: 0.3533 - val_accuracy: 0.7844\n",
      "Epoch 9/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3746 - accuracy: 0.7776 - val_loss: 0.3498 - val_accuracy: 0.7844\n",
      "Epoch 10/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3747 - accuracy: 0.7604 - val_loss: 0.3431 - val_accuracy: 0.7982\n",
      "Epoch 11/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3723 - accuracy: 0.7661 - val_loss: 0.3483 - val_accuracy: 0.7890\n",
      "Epoch 12/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3844 - accuracy: 0.7696 - val_loss: 0.3481 - val_accuracy: 0.7982\n",
      "Epoch 13/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3617 - accuracy: 0.7615 - val_loss: 0.3427 - val_accuracy: 0.7890\n",
      "Epoch 14/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3769 - accuracy: 0.7673 - val_loss: 0.3451 - val_accuracy: 0.7982\n",
      "Epoch 15/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3729 - accuracy: 0.7673 - val_loss: 0.3471 - val_accuracy: 0.8073\n",
      "Epoch 16/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3656 - accuracy: 0.7696 - val_loss: 0.3484 - val_accuracy: 0.7982\n",
      "Epoch 17/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3684 - accuracy: 0.7776 - val_loss: 0.3481 - val_accuracy: 0.7844\n",
      "Epoch 18/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3722 - accuracy: 0.7696 - val_loss: 0.3446 - val_accuracy: 0.8073\n",
      "Epoch 19/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3830 - accuracy: 0.7523 - val_loss: 0.3514 - val_accuracy: 0.8028\n",
      "Epoch 20/5000\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.3695 - accuracy: 0.7673 - val_loss: 0.3496 - val_accuracy: 0.8028\n",
      "Epoch 21/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3630 - accuracy: 0.7730 - val_loss: 0.3500 - val_accuracy: 0.7844\n",
      "Epoch 22/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3652 - accuracy: 0.7650 - val_loss: 0.3477 - val_accuracy: 0.7844\n",
      "Epoch 23/5000\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 0.3655 - accuracy: 0.7776 - val_loss: 0.3508 - val_accuracy: 0.7936\n",
      "Epoch 24/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3604 - accuracy: 0.7730 - val_loss: 0.3482 - val_accuracy: 0.7890\n",
      "Epoch 25/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3677 - accuracy: 0.7661 - val_loss: 0.3489 - val_accuracy: 0.7936\n",
      "Epoch 26/5000\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.3521 - accuracy: 0.7707 - val_loss: 0.3486 - val_accuracy: 0.8073\n",
      "Epoch 27/5000\n",
      "87/87 [==============================] - 1s 17ms/step - loss: 0.3561 - accuracy: 0.7661 - val_loss: 0.3464 - val_accuracy: 0.8073\n",
      "Epoch 28/5000\n",
      "87/87 [==============================] - 1s 12ms/step - loss: 0.3502 - accuracy: 0.7811 - val_loss: 0.3475 - val_accuracy: 0.8028\n",
      "Epoch 29/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3819 - accuracy: 0.7673 - val_loss: 0.3496 - val_accuracy: 0.7890\n",
      "Epoch 30/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3631 - accuracy: 0.7730 - val_loss: 0.3522 - val_accuracy: 0.7936\n",
      "Epoch 31/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3562 - accuracy: 0.7776 - val_loss: 0.3467 - val_accuracy: 0.7982\n",
      "Epoch 32/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3583 - accuracy: 0.7707 - val_loss: 0.3487 - val_accuracy: 0.7936\n",
      "Epoch 33/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3548 - accuracy: 0.7742 - val_loss: 0.3513 - val_accuracy: 0.7890\n",
      "Epoch 34/5000\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3505 - accuracy: 0.7823 - val_loss: 0.3513 - val_accuracy: 0.7936\n",
      "Epoch 35/5000\n",
      "85/87 [============================>.] - ETA: 0s - loss: 0.3636 - accuracy: 0.7718Restoring model weights from the end of the best epoch: 15.\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.3625 - accuracy: 0.7742 - val_loss: 0.3546 - val_accuracy: 0.7890\n",
      "Epoch 35: early stopping\n",
      "Epoch 1/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3782 - accuracy: 0.7270 - val_loss: 0.3704 - val_accuracy: 0.7705\n",
      "Epoch 2/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3667 - accuracy: 0.7641 - val_loss: 0.3806 - val_accuracy: 0.7486\n",
      "Epoch 3/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3601 - accuracy: 0.7805 - val_loss: 0.3863 - val_accuracy: 0.7541\n",
      "Epoch 4/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3545 - accuracy: 0.7860 - val_loss: 0.3662 - val_accuracy: 0.7541\n",
      "Epoch 5/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3548 - accuracy: 0.7888 - val_loss: 0.3511 - val_accuracy: 0.7541\n",
      "Epoch 6/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3620 - accuracy: 0.7888 - val_loss: 0.3695 - val_accuracy: 0.7541\n",
      "Epoch 7/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3622 - accuracy: 0.7819 - val_loss: 0.3602 - val_accuracy: 0.7541\n",
      "Epoch 8/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3586 - accuracy: 0.7901 - val_loss: 0.3708 - val_accuracy: 0.7541\n",
      "Epoch 9/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3561 - accuracy: 0.7915 - val_loss: 0.3512 - val_accuracy: 0.7596\n",
      "Epoch 10/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3637 - accuracy: 0.7846 - val_loss: 0.3725 - val_accuracy: 0.7541\n",
      "Epoch 11/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3604 - accuracy: 0.7846 - val_loss: 0.3534 - val_accuracy: 0.7596\n",
      "Epoch 12/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3609 - accuracy: 0.7860 - val_loss: 0.3546 - val_accuracy: 0.7596\n",
      "Epoch 13/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3604 - accuracy: 0.7833 - val_loss: 0.3574 - val_accuracy: 0.7596\n",
      "Epoch 14/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3724 - accuracy: 0.7874 - val_loss: 0.3548 - val_accuracy: 0.7596\n",
      "Epoch 15/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3534 - accuracy: 0.7915 - val_loss: 0.3502 - val_accuracy: 0.7650\n",
      "Epoch 16/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3809 - accuracy: 0.7860 - val_loss: 0.3607 - val_accuracy: 0.7596\n",
      "Epoch 17/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3531 - accuracy: 0.7901 - val_loss: 0.3552 - val_accuracy: 0.7596\n",
      "Epoch 18/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3652 - accuracy: 0.7901 - val_loss: 0.3590 - val_accuracy: 0.7596\n",
      "Epoch 19/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3558 - accuracy: 0.7860 - val_loss: 0.3548 - val_accuracy: 0.7596\n",
      "Epoch 20/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3573 - accuracy: 0.7915 - val_loss: 0.3557 - val_accuracy: 0.7596\n",
      "Epoch 21/5000\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.7888Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.3598 - accuracy: 0.7888 - val_loss: 0.3549 - val_accuracy: 0.7596\n",
      "Epoch 21: early stopping\n",
      "Epoch 1/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3787 - accuracy: 0.7615 - val_loss: 0.3569 - val_accuracy: 0.7419\n",
      "Epoch 2/5000\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.3626 - accuracy: 0.7642 - val_loss: 0.3535 - val_accuracy: 0.7366\n",
      "Epoch 3/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3749 - accuracy: 0.7534 - val_loss: 0.3578 - val_accuracy: 0.7473\n",
      "Epoch 4/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3661 - accuracy: 0.7547 - val_loss: 0.3551 - val_accuracy: 0.7742\n",
      "Epoch 5/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3723 - accuracy: 0.7709 - val_loss: 0.3559 - val_accuracy: 0.7742\n",
      "Epoch 6/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3703 - accuracy: 0.7655 - val_loss: 0.3564 - val_accuracy: 0.7796\n",
      "Epoch 7/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3666 - accuracy: 0.7628 - val_loss: 0.3533 - val_accuracy: 0.7742\n",
      "Epoch 8/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3716 - accuracy: 0.7655 - val_loss: 0.3583 - val_accuracy: 0.7634\n",
      "Epoch 9/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3806 - accuracy: 0.7736 - val_loss: 0.3551 - val_accuracy: 0.7581\n",
      "Epoch 10/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3688 - accuracy: 0.7588 - val_loss: 0.3575 - val_accuracy: 0.7688\n",
      "Epoch 11/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3654 - accuracy: 0.7709 - val_loss: 0.3538 - val_accuracy: 0.7688\n",
      "Epoch 12/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3734 - accuracy: 0.7736 - val_loss: 0.3562 - val_accuracy: 0.7581\n",
      "Epoch 13/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3716 - accuracy: 0.7803 - val_loss: 0.3564 - val_accuracy: 0.7527\n",
      "Epoch 14/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3559 - accuracy: 0.7722 - val_loss: 0.3551 - val_accuracy: 0.7527\n",
      "Epoch 15/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3567 - accuracy: 0.7817 - val_loss: 0.3537 - val_accuracy: 0.7634\n",
      "Epoch 16/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3603 - accuracy: 0.7722 - val_loss: 0.3578 - val_accuracy: 0.7581\n",
      "Epoch 17/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3637 - accuracy: 0.7709 - val_loss: 0.3591 - val_accuracy: 0.7581\n",
      "Epoch 18/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3552 - accuracy: 0.7844 - val_loss: 0.3608 - val_accuracy: 0.7527\n",
      "Epoch 19/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3791 - accuracy: 0.7695 - val_loss: 0.3593 - val_accuracy: 0.7527\n",
      "Epoch 20/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3598 - accuracy: 0.7871 - val_loss: 0.3547 - val_accuracy: 0.7688\n",
      "Epoch 21/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3668 - accuracy: 0.7857 - val_loss: 0.3531 - val_accuracy: 0.7688\n",
      "Epoch 22/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3641 - accuracy: 0.7736 - val_loss: 0.3544 - val_accuracy: 0.7688\n",
      "Epoch 23/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3800 - accuracy: 0.7763 - val_loss: 0.3623 - val_accuracy: 0.7527\n",
      "Epoch 24/5000\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.3679 - accuracy: 0.7668 - val_loss: 0.3593 - val_accuracy: 0.7634\n",
      "Epoch 25/5000\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3552 - accuracy: 0.7736 - val_loss: 0.3581 - val_accuracy: 0.7527\n",
      "Epoch 26/5000\n",
      "72/75 [===========================>..] - ETA: 0s - loss: 0.3557 - accuracy: 0.7778Restoring model weights from the end of the best epoch: 6.\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.3561 - accuracy: 0.7776 - val_loss: 0.3572 - val_accuracy: 0.7634\n",
      "Epoch 26: early stopping\n",
      "Epoch 1/5000\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.3690 - accuracy: 0.7584 - val_loss: 0.3593 - val_accuracy: 0.7921\n",
      "Epoch 2/5000\n",
      "72/72 [==============================] - 1s 12ms/step - loss: 0.3675 - accuracy: 0.7514 - val_loss: 0.3506 - val_accuracy: 0.7921\n",
      "Epoch 3/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3718 - accuracy: 0.7402 - val_loss: 0.3565 - val_accuracy: 0.7978\n",
      "Epoch 4/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3618 - accuracy: 0.7626 - val_loss: 0.3532 - val_accuracy: 0.7753\n",
      "Epoch 5/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3718 - accuracy: 0.7458 - val_loss: 0.3603 - val_accuracy: 0.8034\n",
      "Epoch 6/5000\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3924 - accuracy: 0.7374 - val_loss: 0.3703 - val_accuracy: 0.7809\n",
      "Epoch 7/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3616 - accuracy: 0.7584 - val_loss: 0.3567 - val_accuracy: 0.8034\n",
      "Epoch 8/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3665 - accuracy: 0.7570 - val_loss: 0.3563 - val_accuracy: 0.8090\n",
      "Epoch 9/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3599 - accuracy: 0.7753 - val_loss: 0.3585 - val_accuracy: 0.7978\n",
      "Epoch 10/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3706 - accuracy: 0.7514 - val_loss: 0.3501 - val_accuracy: 0.7978\n",
      "Epoch 11/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3667 - accuracy: 0.7570 - val_loss: 0.3617 - val_accuracy: 0.7978\n",
      "Epoch 12/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3614 - accuracy: 0.7472 - val_loss: 0.3624 - val_accuracy: 0.8034\n",
      "Epoch 13/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3704 - accuracy: 0.7514 - val_loss: 0.3609 - val_accuracy: 0.8034\n",
      "Epoch 14/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3644 - accuracy: 0.7416 - val_loss: 0.3577 - val_accuracy: 0.8034\n",
      "Epoch 15/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3629 - accuracy: 0.7697 - val_loss: 0.3545 - val_accuracy: 0.7978\n",
      "Epoch 16/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3709 - accuracy: 0.7556 - val_loss: 0.3514 - val_accuracy: 0.8034\n",
      "Epoch 17/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3557 - accuracy: 0.7654 - val_loss: 0.3468 - val_accuracy: 0.8146\n",
      "Epoch 18/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3640 - accuracy: 0.7683 - val_loss: 0.3513 - val_accuracy: 0.8146\n",
      "Epoch 19/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3751 - accuracy: 0.7528 - val_loss: 0.3527 - val_accuracy: 0.8146\n",
      "Epoch 20/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3589 - accuracy: 0.7612 - val_loss: 0.3560 - val_accuracy: 0.7978\n",
      "Epoch 21/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3563 - accuracy: 0.7598 - val_loss: 0.3475 - val_accuracy: 0.8090\n",
      "Epoch 22/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3560 - accuracy: 0.7725 - val_loss: 0.3472 - val_accuracy: 0.8034\n",
      "Epoch 23/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3716 - accuracy: 0.7584 - val_loss: 0.3502 - val_accuracy: 0.8090\n",
      "Epoch 24/5000\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.3614 - accuracy: 0.7697 - val_loss: 0.3552 - val_accuracy: 0.7978\n",
      "Epoch 25/5000\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.3595 - accuracy: 0.7640 - val_loss: 0.3485 - val_accuracy: 0.7978\n",
      "Epoch 26/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3837 - accuracy: 0.7528 - val_loss: 0.3647 - val_accuracy: 0.8090\n",
      "Epoch 27/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3762 - accuracy: 0.7500 - val_loss: 0.3527 - val_accuracy: 0.7921\n",
      "Epoch 28/5000\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3632 - accuracy: 0.7654 - val_loss: 0.3519 - val_accuracy: 0.8034\n",
      "Epoch 29/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3683 - accuracy: 0.7697 - val_loss: 0.3467 - val_accuracy: 0.7978\n",
      "Epoch 30/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3683 - accuracy: 0.7458 - val_loss: 0.3475 - val_accuracy: 0.7809\n",
      "Epoch 31/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3702 - accuracy: 0.7626 - val_loss: 0.3457 - val_accuracy: 0.7865\n",
      "Epoch 32/5000\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.3556 - accuracy: 0.7584 - val_loss: 0.3428 - val_accuracy: 0.7809\n",
      "Epoch 33/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3637 - accuracy: 0.7528 - val_loss: 0.3524 - val_accuracy: 0.7809\n",
      "Epoch 34/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3531 - accuracy: 0.7837 - val_loss: 0.3482 - val_accuracy: 0.7809\n",
      "Epoch 35/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3584 - accuracy: 0.7486 - val_loss: 0.3446 - val_accuracy: 0.8034\n",
      "Epoch 36/5000\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3594 - accuracy: 0.7556 - val_loss: 0.3568 - val_accuracy: 0.8090\n",
      "Epoch 37/5000\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3572 - accuracy: 0.7866Restoring model weights from the end of the best epoch: 17.\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3610 - accuracy: 0.7809 - val_loss: 0.3519 - val_accuracy: 0.8090\n",
      "Epoch 37: early stopping\n",
      "Epoch 1/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3651 - accuracy: 0.7737 - val_loss: 0.3476 - val_accuracy: 0.7720\n",
      "Epoch 2/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3561 - accuracy: 0.7750 - val_loss: 0.3468 - val_accuracy: 0.7772\n",
      "Epoch 3/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3589 - accuracy: 0.7867 - val_loss: 0.3459 - val_accuracy: 0.7824\n",
      "Epoch 4/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3503 - accuracy: 0.7854 - val_loss: 0.3464 - val_accuracy: 0.7772\n",
      "Epoch 5/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3484 - accuracy: 0.7906 - val_loss: 0.3460 - val_accuracy: 0.7824\n",
      "Epoch 6/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3547 - accuracy: 0.7854 - val_loss: 0.3446 - val_accuracy: 0.7824\n",
      "Epoch 7/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3570 - accuracy: 0.7867 - val_loss: 0.3455 - val_accuracy: 0.7824\n",
      "Epoch 8/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3811 - accuracy: 0.7828 - val_loss: 0.3455 - val_accuracy: 0.7824\n",
      "Epoch 9/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3504 - accuracy: 0.7893 - val_loss: 0.3448 - val_accuracy: 0.7824\n",
      "Epoch 10/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3627 - accuracy: 0.7776 - val_loss: 0.3448 - val_accuracy: 0.7824\n",
      "Epoch 11/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3547 - accuracy: 0.7893 - val_loss: 0.3447 - val_accuracy: 0.7824\n",
      "Epoch 12/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3442 - accuracy: 0.7906 - val_loss: 0.3478 - val_accuracy: 0.7824\n",
      "Epoch 13/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3512 - accuracy: 0.7854 - val_loss: 0.3458 - val_accuracy: 0.7824\n",
      "Epoch 14/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3665 - accuracy: 0.7841 - val_loss: 0.3466 - val_accuracy: 0.7824\n",
      "Epoch 15/5000\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.3507 - accuracy: 0.7880 - val_loss: 0.3457 - val_accuracy: 0.7824\n",
      "Epoch 16/5000\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.3567 - accuracy: 0.7867 - val_loss: 0.3459 - val_accuracy: 0.7824\n",
      "Epoch 17/5000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.3578 - accuracy: 0.7815 - val_loss: 0.3446 - val_accuracy: 0.7824\n",
      "Epoch 18/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3482 - accuracy: 0.7880 - val_loss: 0.3462 - val_accuracy: 0.7824\n",
      "Epoch 19/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3541 - accuracy: 0.7789 - val_loss: 0.3443 - val_accuracy: 0.7824\n",
      "Epoch 20/5000\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.3430 - accuracy: 0.7906 - val_loss: 0.3471 - val_accuracy: 0.7824\n",
      "Epoch 21/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3471 - accuracy: 0.7893 - val_loss: 0.3453 - val_accuracy: 0.7824\n",
      "Epoch 22/5000\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3428 - accuracy: 0.7906 - val_loss: 0.3514 - val_accuracy: 0.7720\n",
      "Epoch 23/5000\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.3613 - accuracy: 0.7767Restoring model weights from the end of the best epoch: 3.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.3595 - accuracy: 0.7802 - val_loss: 0.3428 - val_accuracy: 0.7824\n",
      "Epoch 23: early stopping\n",
      "Epoch 1/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3493 - accuracy: 0.8285 - val_loss: 0.3373 - val_accuracy: 0.8138\n",
      "Epoch 2/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3427 - accuracy: 0.8338 - val_loss: 0.3432 - val_accuracy: 0.8138\n",
      "Epoch 3/5000\n",
      "76/76 [==============================] - 1s 11ms/step - loss: 0.3287 - accuracy: 0.8391 - val_loss: 0.3355 - val_accuracy: 0.8138\n",
      "Epoch 4/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3458 - accuracy: 0.8285 - val_loss: 0.3357 - val_accuracy: 0.8191\n",
      "Epoch 5/5000\n",
      "76/76 [==============================] - 1s 11ms/step - loss: 0.3458 - accuracy: 0.8298 - val_loss: 0.3291 - val_accuracy: 0.8191\n",
      "Epoch 6/5000\n",
      "76/76 [==============================] - 1s 11ms/step - loss: 0.3360 - accuracy: 0.8311 - val_loss: 0.3312 - val_accuracy: 0.8138\n",
      "Epoch 7/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3335 - accuracy: 0.8298 - val_loss: 0.3351 - val_accuracy: 0.8138\n",
      "Epoch 8/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3352 - accuracy: 0.8324 - val_loss: 0.3378 - val_accuracy: 0.8191\n",
      "Epoch 9/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3394 - accuracy: 0.8324 - val_loss: 0.3356 - val_accuracy: 0.8138\n",
      "Epoch 10/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3339 - accuracy: 0.8338 - val_loss: 0.3320 - val_accuracy: 0.8191\n",
      "Epoch 11/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3286 - accuracy: 0.8311 - val_loss: 0.3303 - val_accuracy: 0.8138\n",
      "Epoch 12/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3432 - accuracy: 0.8338 - val_loss: 0.3330 - val_accuracy: 0.8138\n",
      "Epoch 13/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3290 - accuracy: 0.8351 - val_loss: 0.3375 - val_accuracy: 0.8138\n",
      "Epoch 14/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3270 - accuracy: 0.8351 - val_loss: 0.3333 - val_accuracy: 0.8138\n",
      "Epoch 15/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3407 - accuracy: 0.8338 - val_loss: 0.3331 - val_accuracy: 0.8191\n",
      "Epoch 16/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3321 - accuracy: 0.8364 - val_loss: 0.3367 - val_accuracy: 0.8138\n",
      "Epoch 17/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3394 - accuracy: 0.8298 - val_loss: 0.3351 - val_accuracy: 0.8138\n",
      "Epoch 18/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3444 - accuracy: 0.8298 - val_loss: 0.3354 - val_accuracy: 0.8138\n",
      "Epoch 19/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3436 - accuracy: 0.8285 - val_loss: 0.3375 - val_accuracy: 0.8138\n",
      "Epoch 20/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3312 - accuracy: 0.8338 - val_loss: 0.3372 - val_accuracy: 0.8138\n",
      "Epoch 21/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3326 - accuracy: 0.8351 - val_loss: 0.3343 - val_accuracy: 0.8138\n",
      "Epoch 22/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3305 - accuracy: 0.8338 - val_loss: 0.3328 - val_accuracy: 0.8138\n",
      "Epoch 23/5000\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3259 - accuracy: 0.8364 - val_loss: 0.3329 - val_accuracy: 0.8191\n",
      "Epoch 24/5000\n",
      "73/76 [===========================>..] - ETA: 0s - loss: 0.3251 - accuracy: 0.8342Restoring model weights from the end of the best epoch: 4.\n",
      "76/76 [==============================] - 1s 10ms/step - loss: 0.3273 - accuracy: 0.8298 - val_loss: 0.3334 - val_accuracy: 0.8138\n",
      "Epoch 24: early stopping\n",
      "Epoch 1/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3906 - accuracy: 0.7477 - val_loss: 0.3592 - val_accuracy: 0.7431\n",
      "Epoch 2/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3691 - accuracy: 0.7638 - val_loss: 0.3588 - val_accuracy: 0.7569\n",
      "Epoch 3/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3706 - accuracy: 0.7603 - val_loss: 0.3569 - val_accuracy: 0.7477\n",
      "Epoch 4/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3674 - accuracy: 0.7672 - val_loss: 0.3559 - val_accuracy: 0.7523\n",
      "Epoch 5/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3819 - accuracy: 0.7615 - val_loss: 0.3561 - val_accuracy: 0.7477\n",
      "Epoch 6/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3737 - accuracy: 0.7706 - val_loss: 0.3551 - val_accuracy: 0.7477\n",
      "Epoch 7/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3624 - accuracy: 0.7638 - val_loss: 0.3548 - val_accuracy: 0.7523\n",
      "Epoch 8/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3724 - accuracy: 0.7580 - val_loss: 0.3548 - val_accuracy: 0.7523\n",
      "Epoch 9/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3628 - accuracy: 0.7649 - val_loss: 0.3518 - val_accuracy: 0.7477\n",
      "Epoch 10/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3749 - accuracy: 0.7534 - val_loss: 0.3553 - val_accuracy: 0.7431\n",
      "Epoch 11/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3632 - accuracy: 0.7741 - val_loss: 0.3529 - val_accuracy: 0.7523\n",
      "Epoch 12/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3588 - accuracy: 0.7661 - val_loss: 0.3522 - val_accuracy: 0.7431\n",
      "Epoch 13/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3659 - accuracy: 0.7706 - val_loss: 0.3520 - val_accuracy: 0.7431\n",
      "Epoch 14/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3661 - accuracy: 0.7615 - val_loss: 0.3515 - val_accuracy: 0.7523\n",
      "Epoch 15/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3715 - accuracy: 0.7775 - val_loss: 0.3532 - val_accuracy: 0.7339\n",
      "Epoch 16/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3559 - accuracy: 0.7764 - val_loss: 0.3534 - val_accuracy: 0.7385\n",
      "Epoch 17/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3528 - accuracy: 0.7672 - val_loss: 0.3522 - val_accuracy: 0.7569\n",
      "Epoch 18/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3650 - accuracy: 0.7603 - val_loss: 0.3535 - val_accuracy: 0.7706\n",
      "Epoch 19/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3569 - accuracy: 0.7706 - val_loss: 0.3530 - val_accuracy: 0.7477\n",
      "Epoch 20/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3601 - accuracy: 0.7741 - val_loss: 0.3524 - val_accuracy: 0.7615\n",
      "Epoch 21/5000\n",
      "88/88 [==============================] - 1s 11ms/step - loss: 0.3640 - accuracy: 0.7683 - val_loss: 0.3546 - val_accuracy: 0.7431\n",
      "Epoch 22/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3683 - accuracy: 0.7741 - val_loss: 0.3583 - val_accuracy: 0.7477\n",
      "Epoch 23/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3632 - accuracy: 0.7775 - val_loss: 0.3555 - val_accuracy: 0.7385\n",
      "Epoch 24/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3534 - accuracy: 0.7810 - val_loss: 0.3526 - val_accuracy: 0.7523\n",
      "Epoch 25/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3565 - accuracy: 0.7661 - val_loss: 0.3528 - val_accuracy: 0.7431\n",
      "Epoch 26/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3660 - accuracy: 0.7798 - val_loss: 0.3532 - val_accuracy: 0.7339\n",
      "Epoch 27/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3606 - accuracy: 0.7741 - val_loss: 0.3575 - val_accuracy: 0.7477\n",
      "Epoch 28/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3606 - accuracy: 0.7810 - val_loss: 0.3567 - val_accuracy: 0.7523\n",
      "Epoch 29/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3554 - accuracy: 0.7706 - val_loss: 0.3568 - val_accuracy: 0.7569\n",
      "Epoch 30/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3612 - accuracy: 0.7638 - val_loss: 0.3594 - val_accuracy: 0.7752\n",
      "Epoch 31/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3836 - accuracy: 0.7649 - val_loss: 0.3599 - val_accuracy: 0.7615\n",
      "Epoch 32/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3581 - accuracy: 0.7695 - val_loss: 0.3573 - val_accuracy: 0.7569\n",
      "Epoch 33/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3722 - accuracy: 0.7683 - val_loss: 0.3559 - val_accuracy: 0.7431\n",
      "Epoch 34/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3612 - accuracy: 0.7775 - val_loss: 0.3577 - val_accuracy: 0.7477\n",
      "Epoch 35/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3610 - accuracy: 0.7729 - val_loss: 0.3552 - val_accuracy: 0.7385\n",
      "Epoch 36/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3559 - accuracy: 0.7775 - val_loss: 0.3554 - val_accuracy: 0.7339\n",
      "Epoch 37/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3548 - accuracy: 0.7729 - val_loss: 0.3551 - val_accuracy: 0.7523\n",
      "Epoch 38/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3524 - accuracy: 0.7706 - val_loss: 0.3545 - val_accuracy: 0.7523\n",
      "Epoch 39/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3508 - accuracy: 0.7844 - val_loss: 0.3564 - val_accuracy: 0.7431\n",
      "Epoch 40/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3608 - accuracy: 0.7718 - val_loss: 0.3544 - val_accuracy: 0.7569\n",
      "Epoch 41/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3576 - accuracy: 0.7729 - val_loss: 0.3556 - val_accuracy: 0.7615\n",
      "Epoch 42/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3562 - accuracy: 0.7798 - val_loss: 0.3571 - val_accuracy: 0.7569\n",
      "Epoch 43/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3631 - accuracy: 0.7787 - val_loss: 0.3576 - val_accuracy: 0.7294\n",
      "Epoch 44/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3575 - accuracy: 0.7787 - val_loss: 0.3539 - val_accuracy: 0.7431\n",
      "Epoch 45/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3527 - accuracy: 0.7821 - val_loss: 0.3525 - val_accuracy: 0.7431\n",
      "Epoch 46/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3540 - accuracy: 0.7672 - val_loss: 0.3530 - val_accuracy: 0.7431\n",
      "Epoch 47/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3557 - accuracy: 0.7764 - val_loss: 0.3506 - val_accuracy: 0.7523\n",
      "Epoch 48/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3557 - accuracy: 0.7924 - val_loss: 0.3505 - val_accuracy: 0.7431\n",
      "Epoch 49/5000\n",
      "88/88 [==============================] - 1s 10ms/step - loss: 0.3524 - accuracy: 0.7867 - val_loss: 0.3524 - val_accuracy: 0.7431\n",
      "Epoch 50/5000\n",
      "86/88 [============================>.] - ETA: 0s - loss: 0.3641 - accuracy: 0.7814Restoring model weights from the end of the best epoch: 30.\n",
      "88/88 [==============================] - 1s 11ms/step - loss: 0.3640 - accuracy: 0.7810 - val_loss: 0.3510 - val_accuracy: 0.7431\n",
      "Epoch 50: early stopping\n",
      "Epoch 1/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3651 - accuracy: 0.7693 - val_loss: 0.3551 - val_accuracy: 0.7321\n",
      "Epoch 2/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3691 - accuracy: 0.7693 - val_loss: 0.3658 - val_accuracy: 0.7381\n",
      "Epoch 3/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3570 - accuracy: 0.7723 - val_loss: 0.3660 - val_accuracy: 0.7143\n",
      "Epoch 4/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3609 - accuracy: 0.7783 - val_loss: 0.3504 - val_accuracy: 0.7381\n",
      "Epoch 5/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3643 - accuracy: 0.7589 - val_loss: 0.3681 - val_accuracy: 0.7202\n",
      "Epoch 6/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3683 - accuracy: 0.7723 - val_loss: 0.3636 - val_accuracy: 0.7262\n",
      "Epoch 7/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3550 - accuracy: 0.7812 - val_loss: 0.3620 - val_accuracy: 0.7262\n",
      "Epoch 8/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3587 - accuracy: 0.7902 - val_loss: 0.3600 - val_accuracy: 0.7381\n",
      "Epoch 9/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3533 - accuracy: 0.7723 - val_loss: 0.3580 - val_accuracy: 0.7321\n",
      "Epoch 10/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3610 - accuracy: 0.7753 - val_loss: 0.3652 - val_accuracy: 0.7202\n",
      "Epoch 11/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3613 - accuracy: 0.7634 - val_loss: 0.3625 - val_accuracy: 0.7202\n",
      "Epoch 12/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3546 - accuracy: 0.7634 - val_loss: 0.3637 - val_accuracy: 0.7262\n",
      "Epoch 13/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3511 - accuracy: 0.7679 - val_loss: 0.3553 - val_accuracy: 0.7381\n",
      "Epoch 14/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3504 - accuracy: 0.7693 - val_loss: 0.3638 - val_accuracy: 0.7202\n",
      "Epoch 15/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3543 - accuracy: 0.7872 - val_loss: 0.3731 - val_accuracy: 0.7202\n",
      "Epoch 16/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3716 - accuracy: 0.7649 - val_loss: 0.3761 - val_accuracy: 0.7202\n",
      "Epoch 17/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3475 - accuracy: 0.7812 - val_loss: 0.3730 - val_accuracy: 0.7143\n",
      "Epoch 18/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3530 - accuracy: 0.7857 - val_loss: 0.3658 - val_accuracy: 0.7202\n",
      "Epoch 19/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3636 - accuracy: 0.7738 - val_loss: 0.3672 - val_accuracy: 0.7202\n",
      "Epoch 20/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3630 - accuracy: 0.7872 - val_loss: 0.3705 - val_accuracy: 0.7202\n",
      "Epoch 21/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3590 - accuracy: 0.7753 - val_loss: 0.3733 - val_accuracy: 0.7202\n",
      "Epoch 22/5000\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.3542 - accuracy: 0.7910Restoring model weights from the end of the best epoch: 2.\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3542 - accuracy: 0.7917 - val_loss: 0.3656 - val_accuracy: 0.7202\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3594 - accuracy: 0.7973 - val_loss: 0.3576 - val_accuracy: 0.7059\n",
      "Epoch 2/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3578 - accuracy: 0.7929 - val_loss: 0.3577 - val_accuracy: 0.7176\n",
      "Epoch 3/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3507 - accuracy: 0.7914 - val_loss: 0.3548 - val_accuracy: 0.7294\n",
      "Epoch 4/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3460 - accuracy: 0.7929 - val_loss: 0.3546 - val_accuracy: 0.7294\n",
      "Epoch 5/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3518 - accuracy: 0.7899 - val_loss: 0.3552 - val_accuracy: 0.7353\n",
      "Epoch 6/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3553 - accuracy: 0.7885 - val_loss: 0.3594 - val_accuracy: 0.7059\n",
      "Epoch 7/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3469 - accuracy: 0.8003 - val_loss: 0.3556 - val_accuracy: 0.7176\n",
      "Epoch 8/5000\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3632 - accuracy: 0.7973 - val_loss: 0.3562 - val_accuracy: 0.7235\n",
      "Epoch 9/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3480 - accuracy: 0.7959 - val_loss: 0.3530 - val_accuracy: 0.7412\n",
      "Epoch 10/5000\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3459 - accuracy: 0.7959 - val_loss: 0.3554 - val_accuracy: 0.7412\n",
      "Epoch 11/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3440 - accuracy: 0.8018 - val_loss: 0.3565 - val_accuracy: 0.7294\n",
      "Epoch 12/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3589 - accuracy: 0.8018 - val_loss: 0.3583 - val_accuracy: 0.7235\n",
      "Epoch 13/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3595 - accuracy: 0.8003 - val_loss: 0.3537 - val_accuracy: 0.7529\n",
      "Epoch 14/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3391 - accuracy: 0.7973 - val_loss: 0.3556 - val_accuracy: 0.7471\n",
      "Epoch 15/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3452 - accuracy: 0.8018 - val_loss: 0.3583 - val_accuracy: 0.7471\n",
      "Epoch 16/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3523 - accuracy: 0.7959 - val_loss: 0.3580 - val_accuracy: 0.7353\n",
      "Epoch 17/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3384 - accuracy: 0.7973 - val_loss: 0.3546 - val_accuracy: 0.7412\n",
      "Epoch 18/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3475 - accuracy: 0.8092 - val_loss: 0.3572 - val_accuracy: 0.7471\n",
      "Epoch 19/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3806 - accuracy: 0.7840 - val_loss: 0.3617 - val_accuracy: 0.7294\n",
      "Epoch 20/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3561 - accuracy: 0.7929 - val_loss: 0.3661 - val_accuracy: 0.7294\n",
      "Epoch 21/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3409 - accuracy: 0.8047 - val_loss: 0.3613 - val_accuracy: 0.7412\n",
      "Epoch 22/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3532 - accuracy: 0.7988 - val_loss: 0.3636 - val_accuracy: 0.7412\n",
      "Epoch 23/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3529 - accuracy: 0.7944 - val_loss: 0.3635 - val_accuracy: 0.7294\n",
      "Epoch 24/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3552 - accuracy: 0.7988 - val_loss: 0.3621 - val_accuracy: 0.7176\n",
      "Epoch 25/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3421 - accuracy: 0.8077 - val_loss: 0.3589 - val_accuracy: 0.7471\n",
      "Epoch 26/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3428 - accuracy: 0.8018 - val_loss: 0.3584 - val_accuracy: 0.7412\n",
      "Epoch 27/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3354 - accuracy: 0.8092 - val_loss: 0.3609 - val_accuracy: 0.7176\n",
      "Epoch 28/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3403 - accuracy: 0.8062 - val_loss: 0.3569 - val_accuracy: 0.7471\n",
      "Epoch 29/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3436 - accuracy: 0.7988 - val_loss: 0.3559 - val_accuracy: 0.7471\n",
      "Epoch 30/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3625 - accuracy: 0.7988 - val_loss: 0.3592 - val_accuracy: 0.7765\n",
      "Epoch 31/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3369 - accuracy: 0.8077 - val_loss: 0.3580 - val_accuracy: 0.7529\n",
      "Epoch 32/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3330 - accuracy: 0.8121 - val_loss: 0.3586 - val_accuracy: 0.7647\n",
      "Epoch 33/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3486 - accuracy: 0.8033 - val_loss: 0.3586 - val_accuracy: 0.7765\n",
      "Epoch 34/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3411 - accuracy: 0.8047 - val_loss: 0.3619 - val_accuracy: 0.7588\n",
      "Epoch 35/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3395 - accuracy: 0.8018 - val_loss: 0.3596 - val_accuracy: 0.7529\n",
      "Epoch 36/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3280 - accuracy: 0.8107 - val_loss: 0.3606 - val_accuracy: 0.7529\n",
      "Epoch 37/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3306 - accuracy: 0.8107 - val_loss: 0.3608 - val_accuracy: 0.7529\n",
      "Epoch 38/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3484 - accuracy: 0.8003 - val_loss: 0.3636 - val_accuracy: 0.7529\n",
      "Epoch 39/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3585 - accuracy: 0.7973 - val_loss: 0.3690 - val_accuracy: 0.7235\n",
      "Epoch 40/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3452 - accuracy: 0.8018 - val_loss: 0.3647 - val_accuracy: 0.7353\n",
      "Epoch 41/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3445 - accuracy: 0.8166 - val_loss: 0.3642 - val_accuracy: 0.7471\n",
      "Epoch 42/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3530 - accuracy: 0.7885 - val_loss: 0.3617 - val_accuracy: 0.7529\n",
      "Epoch 43/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3408 - accuracy: 0.8092 - val_loss: 0.3654 - val_accuracy: 0.7706\n",
      "Epoch 44/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3387 - accuracy: 0.7988 - val_loss: 0.3629 - val_accuracy: 0.7471\n",
      "Epoch 45/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3395 - accuracy: 0.7988 - val_loss: 0.3678 - val_accuracy: 0.7353\n",
      "Epoch 46/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3339 - accuracy: 0.8003 - val_loss: 0.3625 - val_accuracy: 0.7529\n",
      "Epoch 47/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3413 - accuracy: 0.7929 - val_loss: 0.3622 - val_accuracy: 0.7353\n",
      "Epoch 48/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3362 - accuracy: 0.8018 - val_loss: 0.3636 - val_accuracy: 0.7412\n",
      "Epoch 49/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3347 - accuracy: 0.8018 - val_loss: 0.3759 - val_accuracy: 0.7235\n",
      "Epoch 50/5000\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.3581 - accuracy: 0.8045Restoring model weights from the end of the best epoch: 30.\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3581 - accuracy: 0.8033 - val_loss: 0.3680 - val_accuracy: 0.7471\n",
      "Epoch 50: early stopping\n",
      "Epoch 1/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3609 - accuracy: 0.7592 - val_loss: 0.3565 - val_accuracy: 0.7368\n",
      "Epoch 2/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3587 - accuracy: 0.7709 - val_loss: 0.3610 - val_accuracy: 0.7368\n",
      "Epoch 3/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3670 - accuracy: 0.7636 - val_loss: 0.3632 - val_accuracy: 0.7310\n",
      "Epoch 4/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3750 - accuracy: 0.7665 - val_loss: 0.3571 - val_accuracy: 0.7544\n",
      "Epoch 5/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3540 - accuracy: 0.7739 - val_loss: 0.3632 - val_accuracy: 0.7544\n",
      "Epoch 6/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3512 - accuracy: 0.7768 - val_loss: 0.3633 - val_accuracy: 0.7368\n",
      "Epoch 7/5000\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.3522 - accuracy: 0.7680 - val_loss: 0.3637 - val_accuracy: 0.7251\n",
      "Epoch 8/5000\n",
      "69/69 [==============================] - 1s 11ms/step - loss: 0.3617 - accuracy: 0.7930 - val_loss: 0.3651 - val_accuracy: 0.7310\n",
      "Epoch 9/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3619 - accuracy: 0.7665 - val_loss: 0.3617 - val_accuracy: 0.7427\n",
      "Epoch 10/5000\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3536 - accuracy: 0.7680 - val_loss: 0.3586 - val_accuracy: 0.7427\n",
      "Epoch 11/5000\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3648 - accuracy: 0.7695 - val_loss: 0.3615 - val_accuracy: 0.7544\n",
      "Epoch 12/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3612 - accuracy: 0.7695 - val_loss: 0.3584 - val_accuracy: 0.7427\n",
      "Epoch 13/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3464 - accuracy: 0.7856 - val_loss: 0.3605 - val_accuracy: 0.7368\n",
      "Epoch 14/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3587 - accuracy: 0.7812 - val_loss: 0.3579 - val_accuracy: 0.7427\n",
      "Epoch 15/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3607 - accuracy: 0.7739 - val_loss: 0.3603 - val_accuracy: 0.7427\n",
      "Epoch 16/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3621 - accuracy: 0.7812 - val_loss: 0.3678 - val_accuracy: 0.7368\n",
      "Epoch 17/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3631 - accuracy: 0.7724 - val_loss: 0.3712 - val_accuracy: 0.7310\n",
      "Epoch 18/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3649 - accuracy: 0.7606 - val_loss: 0.3620 - val_accuracy: 0.7544\n",
      "Epoch 19/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3515 - accuracy: 0.7812 - val_loss: 0.3622 - val_accuracy: 0.7485\n",
      "Epoch 20/5000\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3585 - accuracy: 0.7827 - val_loss: 0.3592 - val_accuracy: 0.7485\n",
      "Epoch 21/5000\n",
      "69/69 [==============================] - 1s 9ms/step - loss: 0.3480 - accuracy: 0.7812 - val_loss: 0.3602 - val_accuracy: 0.7368\n",
      "Epoch 22/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3588 - accuracy: 0.7709 - val_loss: 0.3560 - val_accuracy: 0.7485\n",
      "Epoch 23/5000\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3508 - accuracy: 0.7900 - val_loss: 0.3539 - val_accuracy: 0.7427\n",
      "Epoch 24/5000\n",
      "67/69 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.7716Restoring model weights from the end of the best epoch: 4.\n",
      "69/69 [==============================] - 1s 10ms/step - loss: 0.3597 - accuracy: 0.7695 - val_loss: 0.3574 - val_accuracy: 0.7427\n",
      "Epoch 24: early stopping\n",
      "Epoch 1/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3538 - accuracy: 0.8029 - val_loss: 0.3513 - val_accuracy: 0.7765\n",
      "Epoch 2/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3498 - accuracy: 0.8015 - val_loss: 0.3521 - val_accuracy: 0.7765\n",
      "Epoch 3/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3452 - accuracy: 0.8029 - val_loss: 0.3539 - val_accuracy: 0.7765\n",
      "Epoch 4/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3417 - accuracy: 0.8088 - val_loss: 0.3584 - val_accuracy: 0.7765\n",
      "Epoch 5/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3477 - accuracy: 0.8059 - val_loss: 0.3588 - val_accuracy: 0.7765\n",
      "Epoch 6/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3530 - accuracy: 0.8044 - val_loss: 0.3590 - val_accuracy: 0.7765\n",
      "Epoch 7/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3396 - accuracy: 0.8103 - val_loss: 0.3576 - val_accuracy: 0.7765\n",
      "Epoch 8/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3403 - accuracy: 0.8103 - val_loss: 0.3576 - val_accuracy: 0.7765\n",
      "Epoch 9/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3363 - accuracy: 0.8103 - val_loss: 0.3573 - val_accuracy: 0.7765\n",
      "Epoch 10/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3550 - accuracy: 0.8059 - val_loss: 0.3599 - val_accuracy: 0.7765\n",
      "Epoch 11/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3582 - accuracy: 0.7971 - val_loss: 0.3719 - val_accuracy: 0.7706\n",
      "Epoch 12/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3397 - accuracy: 0.8044 - val_loss: 0.3608 - val_accuracy: 0.7765\n",
      "Epoch 13/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3542 - accuracy: 0.8044 - val_loss: 0.3628 - val_accuracy: 0.7765\n",
      "Epoch 14/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3460 - accuracy: 0.8044 - val_loss: 0.3606 - val_accuracy: 0.7765\n",
      "Epoch 15/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3463 - accuracy: 0.8103 - val_loss: 0.3585 - val_accuracy: 0.7765\n",
      "Epoch 16/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3555 - accuracy: 0.8088 - val_loss: 0.3641 - val_accuracy: 0.7765\n",
      "Epoch 17/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3407 - accuracy: 0.8044 - val_loss: 0.3608 - val_accuracy: 0.7765\n",
      "Epoch 18/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3490 - accuracy: 0.8088 - val_loss: 0.3606 - val_accuracy: 0.7765\n",
      "Epoch 19/5000\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3413 - accuracy: 0.8088 - val_loss: 0.3602 - val_accuracy: 0.7765\n",
      "Epoch 20/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3335 - accuracy: 0.8074 - val_loss: 0.3599 - val_accuracy: 0.7706\n",
      "Epoch 21/5000\n",
      "67/68 [============================>.] - ETA: 0s - loss: 0.3537 - accuracy: 0.8075Restoring model weights from the end of the best epoch: 1.\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 0.3556 - accuracy: 0.8059 - val_loss: 0.3578 - val_accuracy: 0.7706\n",
      "Epoch 21: early stopping\n",
      "Epoch 1/5000\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 0.3798 - accuracy: 0.7131 - val_loss: 0.3676 - val_accuracy: 0.7514\n",
      "Epoch 2/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3693 - accuracy: 0.7267 - val_loss: 0.3492 - val_accuracy: 0.8162\n",
      "Epoch 3/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3560 - accuracy: 0.7754 - val_loss: 0.3438 - val_accuracy: 0.8432\n",
      "Epoch 4/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3531 - accuracy: 0.8038 - val_loss: 0.3413 - val_accuracy: 0.8432\n",
      "Epoch 5/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3563 - accuracy: 0.7997 - val_loss: 0.3414 - val_accuracy: 0.8432\n",
      "Epoch 6/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3566 - accuracy: 0.8065 - val_loss: 0.3388 - val_accuracy: 0.8432\n",
      "Epoch 7/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3434 - accuracy: 0.8092 - val_loss: 0.3353 - val_accuracy: 0.8432\n",
      "Epoch 8/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3450 - accuracy: 0.8078 - val_loss: 0.3359 - val_accuracy: 0.8432\n",
      "Epoch 9/5000\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 0.3460 - accuracy: 0.8038 - val_loss: 0.3297 - val_accuracy: 0.8432\n",
      "Epoch 10/5000\n",
      "74/74 [==============================] - 1s 11ms/step - loss: 0.3490 - accuracy: 0.8078 - val_loss: 0.3354 - val_accuracy: 0.8432\n",
      "Epoch 11/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3510 - accuracy: 0.8078 - val_loss: 0.3345 - val_accuracy: 0.8432\n",
      "Epoch 12/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3568 - accuracy: 0.8078 - val_loss: 0.3337 - val_accuracy: 0.8432\n",
      "Epoch 13/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3405 - accuracy: 0.8119 - val_loss: 0.3306 - val_accuracy: 0.8432\n",
      "Epoch 14/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3462 - accuracy: 0.8065 - val_loss: 0.3282 - val_accuracy: 0.8432\n",
      "Epoch 15/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3473 - accuracy: 0.8024 - val_loss: 0.3325 - val_accuracy: 0.8432\n",
      "Epoch 16/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3504 - accuracy: 0.8051 - val_loss: 0.3307 - val_accuracy: 0.8432\n",
      "Epoch 17/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3431 - accuracy: 0.8106 - val_loss: 0.3351 - val_accuracy: 0.8432\n",
      "Epoch 18/5000\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3448 - accuracy: 0.8065 - val_loss: 0.3330 - val_accuracy: 0.8432\n",
      "Epoch 19/5000\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 0.3494 - accuracy: 0.8078 - val_loss: 0.3342 - val_accuracy: 0.8432\n",
      "Epoch 20/5000\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 0.3446 - accuracy: 0.8065 - val_loss: 0.3345 - val_accuracy: 0.8432\n",
      "Epoch 21/5000\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 0.3396 - accuracy: 0.8106 - val_loss: 0.3361 - val_accuracy: 0.8432\n",
      "Epoch 22/5000\n",
      "74/74 [==============================] - 1s 9ms/step - loss: 0.3404 - accuracy: 0.8051 - val_loss: 0.3330 - val_accuracy: 0.8432\n",
      "Epoch 23/5000\n",
      "71/74 [===========================>..] - ETA: 0s - loss: 0.3443 - accuracy: 0.8070Restoring model weights from the end of the best epoch: 3.\n",
      "74/74 [==============================] - 1s 10ms/step - loss: 0.3449 - accuracy: 0.8051 - val_loss: 0.3311 - val_accuracy: 0.8432\n",
      "Epoch 23: early stopping\n",
      "Epoch 1/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3611 - accuracy: 0.7815 - val_loss: 0.3499 - val_accuracy: 0.7624\n",
      "Epoch 2/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3758 - accuracy: 0.7967 - val_loss: 0.3580 - val_accuracy: 0.7624\n",
      "Epoch 3/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3593 - accuracy: 0.7967 - val_loss: 0.3552 - val_accuracy: 0.7680\n",
      "Epoch 4/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3477 - accuracy: 0.7884 - val_loss: 0.3490 - val_accuracy: 0.7735\n",
      "Epoch 5/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3569 - accuracy: 0.8119 - val_loss: 0.3541 - val_accuracy: 0.7790\n",
      "Epoch 6/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3491 - accuracy: 0.7815 - val_loss: 0.3589 - val_accuracy: 0.7735\n",
      "Epoch 7/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3377 - accuracy: 0.8257 - val_loss: 0.3601 - val_accuracy: 0.7680\n",
      "Epoch 8/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3624 - accuracy: 0.7967 - val_loss: 0.3574 - val_accuracy: 0.7514\n",
      "Epoch 9/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3568 - accuracy: 0.8119 - val_loss: 0.3561 - val_accuracy: 0.7790\n",
      "Epoch 10/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3354 - accuracy: 0.8064 - val_loss: 0.3555 - val_accuracy: 0.7735\n",
      "Epoch 11/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3485 - accuracy: 0.8050 - val_loss: 0.3577 - val_accuracy: 0.7735\n",
      "Epoch 12/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3751 - accuracy: 0.8119 - val_loss: 0.3621 - val_accuracy: 0.7790\n",
      "Epoch 13/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3496 - accuracy: 0.8147 - val_loss: 0.3542 - val_accuracy: 0.7956\n",
      "Epoch 14/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3420 - accuracy: 0.8091 - val_loss: 0.3583 - val_accuracy: 0.7956\n",
      "Epoch 15/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3555 - accuracy: 0.8036 - val_loss: 0.3570 - val_accuracy: 0.7790\n",
      "Epoch 16/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3347 - accuracy: 0.7953 - val_loss: 0.3574 - val_accuracy: 0.7735\n",
      "Epoch 17/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3599 - accuracy: 0.8077 - val_loss: 0.3612 - val_accuracy: 0.7735\n",
      "Epoch 18/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3530 - accuracy: 0.8077 - val_loss: 0.3591 - val_accuracy: 0.7790\n",
      "Epoch 19/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3458 - accuracy: 0.8050 - val_loss: 0.3622 - val_accuracy: 0.7790\n",
      "Epoch 20/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3345 - accuracy: 0.8022 - val_loss: 0.3607 - val_accuracy: 0.7790\n",
      "Epoch 21/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3278 - accuracy: 0.8147 - val_loss: 0.3595 - val_accuracy: 0.7901\n",
      "Epoch 22/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3440 - accuracy: 0.8091 - val_loss: 0.3741 - val_accuracy: 0.7624\n",
      "Epoch 23/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3336 - accuracy: 0.8188 - val_loss: 0.3599 - val_accuracy: 0.7956\n",
      "Epoch 24/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3348 - accuracy: 0.8174 - val_loss: 0.3672 - val_accuracy: 0.7790\n",
      "Epoch 25/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3576 - accuracy: 0.8174 - val_loss: 0.3700 - val_accuracy: 0.7956\n",
      "Epoch 26/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3352 - accuracy: 0.8216 - val_loss: 0.3692 - val_accuracy: 0.7845\n",
      "Epoch 27/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3356 - accuracy: 0.8174 - val_loss: 0.3629 - val_accuracy: 0.7845\n",
      "Epoch 28/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3409 - accuracy: 0.8188 - val_loss: 0.3723 - val_accuracy: 0.7790\n",
      "Epoch 29/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3301 - accuracy: 0.8243 - val_loss: 0.3728 - val_accuracy: 0.7845\n",
      "Epoch 30/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3376 - accuracy: 0.8271 - val_loss: 0.3591 - val_accuracy: 0.7956\n",
      "Epoch 31/5000\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.3398 - accuracy: 0.8077 - val_loss: 0.3635 - val_accuracy: 0.7735\n",
      "Epoch 32/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3374 - accuracy: 0.8230 - val_loss: 0.3655 - val_accuracy: 0.7735\n",
      "Epoch 33/5000\n",
      "70/73 [===========================>..] - ETA: 0s - loss: 0.3269 - accuracy: 0.8214Restoring model weights from the end of the best epoch: 13.\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3268 - accuracy: 0.8216 - val_loss: 0.3633 - val_accuracy: 0.7845\n",
      "Epoch 33: early stopping\n",
      "Epoch 1/5000\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.3759 - accuracy: 0.7613 - val_loss: 0.3531 - val_accuracy: 0.7865\n",
      "Epoch 2/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3688 - accuracy: 0.7684 - val_loss: 0.3550 - val_accuracy: 0.7753\n",
      "Epoch 3/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3663 - accuracy: 0.7599 - val_loss: 0.3528 - val_accuracy: 0.7865\n",
      "Epoch 4/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3665 - accuracy: 0.7585 - val_loss: 0.3518 - val_accuracy: 0.8034\n",
      "Epoch 5/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3510 - accuracy: 0.7740 - val_loss: 0.3507 - val_accuracy: 0.7865\n",
      "Epoch 6/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3520 - accuracy: 0.7782 - val_loss: 0.3559 - val_accuracy: 0.7584\n",
      "Epoch 7/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3688 - accuracy: 0.7387 - val_loss: 0.3539 - val_accuracy: 0.7640\n",
      "Epoch 8/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3642 - accuracy: 0.7387 - val_loss: 0.3531 - val_accuracy: 0.7978\n",
      "Epoch 9/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3600 - accuracy: 0.7571 - val_loss: 0.3555 - val_accuracy: 0.7528\n",
      "Epoch 10/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3633 - accuracy: 0.7486 - val_loss: 0.3526 - val_accuracy: 0.7978\n",
      "Epoch 11/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3545 - accuracy: 0.7401 - val_loss: 0.3496 - val_accuracy: 0.7753\n",
      "Epoch 12/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3574 - accuracy: 0.7797 - val_loss: 0.3493 - val_accuracy: 0.7865\n",
      "Epoch 13/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3605 - accuracy: 0.7542 - val_loss: 0.3512 - val_accuracy: 0.7697\n",
      "Epoch 14/5000\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.3634 - accuracy: 0.7599 - val_loss: 0.3595 - val_accuracy: 0.7640\n",
      "Epoch 15/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3661 - accuracy: 0.7542 - val_loss: 0.3508 - val_accuracy: 0.7753\n",
      "Epoch 16/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3590 - accuracy: 0.7825 - val_loss: 0.3548 - val_accuracy: 0.7528\n",
      "Epoch 17/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3583 - accuracy: 0.7669 - val_loss: 0.3556 - val_accuracy: 0.7584\n",
      "Epoch 18/5000\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.3557 - accuracy: 0.7599 - val_loss: 0.3564 - val_accuracy: 0.7584\n",
      "Epoch 19/5000\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.3524 - accuracy: 0.7528 - val_loss: 0.3562 - val_accuracy: 0.7640\n",
      "Epoch 20/5000\n",
      "71/71 [==============================] - 1s 17ms/step - loss: 0.3566 - accuracy: 0.7627 - val_loss: 0.3536 - val_accuracy: 0.7865\n",
      "Epoch 21/5000\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.3652 - accuracy: 0.7415 - val_loss: 0.3574 - val_accuracy: 0.7640\n",
      "Epoch 22/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3603 - accuracy: 0.7528 - val_loss: 0.3594 - val_accuracy: 0.7528\n",
      "Epoch 23/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3638 - accuracy: 0.7585 - val_loss: 0.3629 - val_accuracy: 0.7584\n",
      "Epoch 24/5000\n",
      "70/71 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.7571Restoring model weights from the end of the best epoch: 4.\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3570 - accuracy: 0.7599 - val_loss: 0.3618 - val_accuracy: 0.7472\n",
      "Epoch 24: early stopping\n",
      "Epoch 1/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3611 - accuracy: 0.7796 - val_loss: 0.3534 - val_accuracy: 0.7396\n",
      "Epoch 2/5000\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.3652 - accuracy: 0.7870 - val_loss: 0.3524 - val_accuracy: 0.7278\n",
      "Epoch 3/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3629 - accuracy: 0.7678 - val_loss: 0.3557 - val_accuracy: 0.6923\n",
      "Epoch 4/5000\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3499 - accuracy: 0.7751 - val_loss: 0.3615 - val_accuracy: 0.7041\n",
      "Epoch 5/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3483 - accuracy: 0.7929 - val_loss: 0.3676 - val_accuracy: 0.6982\n",
      "Epoch 6/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3656 - accuracy: 0.7751 - val_loss: 0.3558 - val_accuracy: 0.6982\n",
      "Epoch 7/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3467 - accuracy: 0.7973 - val_loss: 0.3594 - val_accuracy: 0.6923\n",
      "Epoch 8/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3491 - accuracy: 0.7840 - val_loss: 0.3597 - val_accuracy: 0.7041\n",
      "Epoch 9/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3532 - accuracy: 0.7633 - val_loss: 0.3697 - val_accuracy: 0.6982\n",
      "Epoch 10/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3564 - accuracy: 0.7885 - val_loss: 0.3702 - val_accuracy: 0.6864\n",
      "Epoch 11/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3561 - accuracy: 0.7870 - val_loss: 0.3611 - val_accuracy: 0.6923\n",
      "Epoch 12/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3478 - accuracy: 0.7870 - val_loss: 0.3631 - val_accuracy: 0.6864\n",
      "Epoch 13/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3394 - accuracy: 0.7855 - val_loss: 0.3614 - val_accuracy: 0.7041\n",
      "Epoch 14/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3490 - accuracy: 0.7825 - val_loss: 0.3625 - val_accuracy: 0.6864\n",
      "Epoch 15/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3641 - accuracy: 0.7796 - val_loss: 0.3597 - val_accuracy: 0.6923\n",
      "Epoch 16/5000\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3540 - accuracy: 0.7781 - val_loss: 0.3622 - val_accuracy: 0.6982\n",
      "Epoch 17/5000\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3538 - accuracy: 0.7766 - val_loss: 0.3612 - val_accuracy: 0.6923\n",
      "Epoch 18/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3501 - accuracy: 0.7885 - val_loss: 0.3646 - val_accuracy: 0.6982\n",
      "Epoch 19/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3496 - accuracy: 0.7781 - val_loss: 0.3606 - val_accuracy: 0.7041\n",
      "Epoch 20/5000\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3561 - accuracy: 0.7766 - val_loss: 0.3608 - val_accuracy: 0.7041\n",
      "Epoch 21/5000\n",
      "63/68 [==========================>...] - ETA: 0s - loss: 0.3611 - accuracy: 0.7778Restoring model weights from the end of the best epoch: 1.\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3591 - accuracy: 0.7781 - val_loss: 0.3591 - val_accuracy: 0.7101\n",
      "Epoch 21: early stopping\n",
      "Epoch 1/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3596 - accuracy: 0.7603 - val_loss: 0.3371 - val_accuracy: 0.8294\n",
      "Epoch 2/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3503 - accuracy: 0.7676 - val_loss: 0.3299 - val_accuracy: 0.8765\n",
      "Epoch 3/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3587 - accuracy: 0.7809 - val_loss: 0.3361 - val_accuracy: 0.8706\n",
      "Epoch 4/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3690 - accuracy: 0.7500 - val_loss: 0.3494 - val_accuracy: 0.8647\n",
      "Epoch 5/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3603 - accuracy: 0.7574 - val_loss: 0.3437 - val_accuracy: 0.8765\n",
      "Epoch 6/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3605 - accuracy: 0.7632 - val_loss: 0.3436 - val_accuracy: 0.8765\n",
      "Epoch 7/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3629 - accuracy: 0.7691 - val_loss: 0.3404 - val_accuracy: 0.8765\n",
      "Epoch 8/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3585 - accuracy: 0.7618 - val_loss: 0.3359 - val_accuracy: 0.8706\n",
      "Epoch 9/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3608 - accuracy: 0.7588 - val_loss: 0.3444 - val_accuracy: 0.8706\n",
      "Epoch 10/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3611 - accuracy: 0.7735 - val_loss: 0.3344 - val_accuracy: 0.8706\n",
      "Epoch 11/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3541 - accuracy: 0.7603 - val_loss: 0.3241 - val_accuracy: 0.8706\n",
      "Epoch 12/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3680 - accuracy: 0.7603 - val_loss: 0.3416 - val_accuracy: 0.8706\n",
      "Epoch 13/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3607 - accuracy: 0.7735 - val_loss: 0.3532 - val_accuracy: 0.8706\n",
      "Epoch 14/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3786 - accuracy: 0.7662 - val_loss: 0.3538 - val_accuracy: 0.8706\n",
      "Epoch 15/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3602 - accuracy: 0.7618 - val_loss: 0.3502 - val_accuracy: 0.8706\n",
      "Epoch 16/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3557 - accuracy: 0.7632 - val_loss: 0.3489 - val_accuracy: 0.8765\n",
      "Epoch 17/5000\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3554 - accuracy: 0.7779 - val_loss: 0.3457 - val_accuracy: 0.8706\n",
      "Epoch 18/5000\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3570 - accuracy: 0.7647 - val_loss: 0.3468 - val_accuracy: 0.8765\n",
      "Epoch 19/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3479 - accuracy: 0.7676 - val_loss: 0.3485 - val_accuracy: 0.8765\n",
      "Epoch 20/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3591 - accuracy: 0.7721 - val_loss: 0.3450 - val_accuracy: 0.8706\n",
      "Epoch 21/5000\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3518 - accuracy: 0.7868 - val_loss: 0.3494 - val_accuracy: 0.8706\n",
      "Epoch 22/5000\n",
      "65/68 [===========================>..] - ETA: 0s - loss: 0.3818 - accuracy: 0.7600Restoring model weights from the end of the best epoch: 2.\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3813 - accuracy: 0.7618 - val_loss: 0.3439 - val_accuracy: 0.8706\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/5000\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.3620 - accuracy: 0.7555 - val_loss: 0.3407 - val_accuracy: 0.7912\n",
      "Epoch 2/5000\n",
      "73/73 [==============================] - 1s 13ms/step - loss: 0.3547 - accuracy: 0.8036 - val_loss: 0.3445 - val_accuracy: 0.7692\n",
      "Epoch 3/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3501 - accuracy: 0.7926 - val_loss: 0.3482 - val_accuracy: 0.7582\n",
      "Epoch 4/5000\n",
      "73/73 [==============================] - 1s 12ms/step - loss: 0.3446 - accuracy: 0.8036 - val_loss: 0.3498 - val_accuracy: 0.7418\n",
      "Epoch 5/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3489 - accuracy: 0.8077 - val_loss: 0.3506 - val_accuracy: 0.7473\n",
      "Epoch 6/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3565 - accuracy: 0.8049 - val_loss: 0.3522 - val_accuracy: 0.7582\n",
      "Epoch 7/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3493 - accuracy: 0.8091 - val_loss: 0.3488 - val_accuracy: 0.7527\n",
      "Epoch 8/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3489 - accuracy: 0.7981 - val_loss: 0.3519 - val_accuracy: 0.7473\n",
      "Epoch 9/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3498 - accuracy: 0.7871 - val_loss: 0.3554 - val_accuracy: 0.7527\n",
      "Epoch 10/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3467 - accuracy: 0.8036 - val_loss: 0.3540 - val_accuracy: 0.7637\n",
      "Epoch 11/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3502 - accuracy: 0.7981 - val_loss: 0.3551 - val_accuracy: 0.7637\n",
      "Epoch 12/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3357 - accuracy: 0.8049 - val_loss: 0.3547 - val_accuracy: 0.7582\n",
      "Epoch 13/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3336 - accuracy: 0.8132 - val_loss: 0.3484 - val_accuracy: 0.7582\n",
      "Epoch 14/5000\n",
      "73/73 [==============================] - 1s 9ms/step - loss: 0.3400 - accuracy: 0.8022 - val_loss: 0.3508 - val_accuracy: 0.7692\n",
      "Epoch 15/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3483 - accuracy: 0.7967 - val_loss: 0.3488 - val_accuracy: 0.7418\n",
      "Epoch 16/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3415 - accuracy: 0.8049 - val_loss: 0.3492 - val_accuracy: 0.7582\n",
      "Epoch 17/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3445 - accuracy: 0.7967 - val_loss: 0.3537 - val_accuracy: 0.7527\n",
      "Epoch 18/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3324 - accuracy: 0.8036 - val_loss: 0.3557 - val_accuracy: 0.7473\n",
      "Epoch 19/5000\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3327 - accuracy: 0.8159 - val_loss: 0.3521 - val_accuracy: 0.7637\n",
      "Epoch 20/5000\n",
      "73/73 [==============================] - 1s 11ms/step - loss: 0.3480 - accuracy: 0.8146 - val_loss: 0.3506 - val_accuracy: 0.7582\n",
      "Epoch 21/5000\n",
      "72/73 [============================>.] - ETA: 0s - loss: 0.3414 - accuracy: 0.7972Restoring model weights from the end of the best epoch: 1.\n",
      "73/73 [==============================] - 1s 10ms/step - loss: 0.3407 - accuracy: 0.7981 - val_loss: 0.3534 - val_accuracy: 0.7582\n",
      "Epoch 21: early stopping\n",
      "Epoch 1/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3675 - accuracy: 0.7841 - val_loss: 0.3631 - val_accuracy: 0.7216\n",
      "Epoch 2/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3492 - accuracy: 0.8026 - val_loss: 0.3704 - val_accuracy: 0.7045\n",
      "Epoch 3/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3490 - accuracy: 0.8054 - val_loss: 0.3724 - val_accuracy: 0.7216\n",
      "Epoch 4/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3493 - accuracy: 0.8011 - val_loss: 0.3698 - val_accuracy: 0.7216\n",
      "Epoch 5/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3375 - accuracy: 0.8054 - val_loss: 0.3710 - val_accuracy: 0.7330\n",
      "Epoch 6/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3543 - accuracy: 0.7969 - val_loss: 0.3706 - val_accuracy: 0.7273\n",
      "Epoch 7/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3476 - accuracy: 0.8026 - val_loss: 0.3703 - val_accuracy: 0.7273\n",
      "Epoch 8/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3587 - accuracy: 0.8011 - val_loss: 0.3682 - val_accuracy: 0.7273\n",
      "Epoch 9/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3439 - accuracy: 0.8026 - val_loss: 0.3686 - val_accuracy: 0.7273\n",
      "Epoch 10/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3384 - accuracy: 0.8011 - val_loss: 0.3691 - val_accuracy: 0.7273\n",
      "Epoch 11/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3524 - accuracy: 0.8026 - val_loss: 0.3706 - val_accuracy: 0.7273\n",
      "Epoch 12/5000\n",
      "71/71 [==============================] - 1s 11ms/step - loss: 0.3470 - accuracy: 0.7997 - val_loss: 0.3713 - val_accuracy: 0.7273\n",
      "Epoch 13/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3465 - accuracy: 0.7997 - val_loss: 0.3735 - val_accuracy: 0.7216\n",
      "Epoch 14/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3409 - accuracy: 0.8054 - val_loss: 0.3718 - val_accuracy: 0.7216\n",
      "Epoch 15/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3562 - accuracy: 0.8097 - val_loss: 0.3728 - val_accuracy: 0.7216\n",
      "Epoch 16/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3545 - accuracy: 0.8097 - val_loss: 0.3748 - val_accuracy: 0.7216\n",
      "Epoch 17/5000\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.3464 - accuracy: 0.8082 - val_loss: 0.3758 - val_accuracy: 0.7216\n",
      "Epoch 18/5000\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.3548 - accuracy: 0.8040 - val_loss: 0.3824 - val_accuracy: 0.7273\n",
      "Epoch 19/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3551 - accuracy: 0.8026 - val_loss: 0.3785 - val_accuracy: 0.7273\n",
      "Epoch 20/5000\n",
      "71/71 [==============================] - 1s 12ms/step - loss: 0.3513 - accuracy: 0.7983 - val_loss: 0.3776 - val_accuracy: 0.7273\n",
      "Epoch 21/5000\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.3366 - accuracy: 0.8097 - val_loss: 0.3728 - val_accuracy: 0.7273\n",
      "Epoch 22/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3449 - accuracy: 0.7997 - val_loss: 0.3734 - val_accuracy: 0.7273\n",
      "Epoch 23/5000\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.3504 - accuracy: 0.8068 - val_loss: 0.3768 - val_accuracy: 0.7216\n",
      "Epoch 24/5000\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3387 - accuracy: 0.8097 - val_loss: 0.3785 - val_accuracy: 0.7273\n",
      "Epoch 25/5000\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 0.3609 - accuracy: 0.8029Restoring model weights from the end of the best epoch: 5.\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.3595 - accuracy: 0.8026 - val_loss: 0.3765 - val_accuracy: 0.7216\n",
      "Epoch 25: early stopping\n",
      "Epoch 1/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3768 - accuracy: 0.7517 - val_loss: 0.3547 - val_accuracy: 0.7644\n",
      "Epoch 2/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3518 - accuracy: 0.7617 - val_loss: 0.3461 - val_accuracy: 0.7778\n",
      "Epoch 3/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3661 - accuracy: 0.7728 - val_loss: 0.3447 - val_accuracy: 0.7600\n",
      "Epoch 4/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3556 - accuracy: 0.7717 - val_loss: 0.3455 - val_accuracy: 0.7556\n",
      "Epoch 5/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3472 - accuracy: 0.7873 - val_loss: 0.3444 - val_accuracy: 0.7600\n",
      "Epoch 6/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3547 - accuracy: 0.7739 - val_loss: 0.3434 - val_accuracy: 0.7600\n",
      "Epoch 7/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3533 - accuracy: 0.7639 - val_loss: 0.3440 - val_accuracy: 0.7600\n",
      "Epoch 8/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3577 - accuracy: 0.7695 - val_loss: 0.3432 - val_accuracy: 0.7689\n",
      "Epoch 9/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3548 - accuracy: 0.7684 - val_loss: 0.3414 - val_accuracy: 0.7689\n",
      "Epoch 10/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3513 - accuracy: 0.7773 - val_loss: 0.3422 - val_accuracy: 0.7644\n",
      "Epoch 11/5000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.3561 - accuracy: 0.7840 - val_loss: 0.3395 - val_accuracy: 0.7556\n",
      "Epoch 12/5000\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.3656 - accuracy: 0.7739 - val_loss: 0.3461 - val_accuracy: 0.7556\n",
      "Epoch 13/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3620 - accuracy: 0.7817 - val_loss: 0.3452 - val_accuracy: 0.7644\n",
      "Epoch 14/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3484 - accuracy: 0.7817 - val_loss: 0.3415 - val_accuracy: 0.7644\n",
      "Epoch 15/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3483 - accuracy: 0.7840 - val_loss: 0.3404 - val_accuracy: 0.7644\n",
      "Epoch 16/5000\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.3609 - accuracy: 0.7751 - val_loss: 0.3439 - val_accuracy: 0.7733\n",
      "Epoch 17/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3541 - accuracy: 0.7773 - val_loss: 0.3456 - val_accuracy: 0.7689\n",
      "Epoch 18/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3601 - accuracy: 0.7806 - val_loss: 0.3516 - val_accuracy: 0.7733\n",
      "Epoch 19/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3677 - accuracy: 0.7840 - val_loss: 0.3424 - val_accuracy: 0.7644\n",
      "Epoch 20/5000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.3456 - accuracy: 0.7873 - val_loss: 0.3428 - val_accuracy: 0.7733\n",
      "Epoch 21/5000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3474 - accuracy: 0.7873 - val_loss: 0.3447 - val_accuracy: 0.7733\n",
      "Epoch 22/5000\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.3491 - accuracy: 0.7812Restoring model weights from the end of the best epoch: 2.\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.3467 - accuracy: 0.7840 - val_loss: 0.3445 - val_accuracy: 0.7778\n",
      "Epoch 22: early stopping\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import time\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "val_accuracies = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for team in teams:\n",
    "    data = df[df['TEAM_ABBREVIATION_H'] == team]\n",
    "    data = data.drop('TEAM_ABBREVIATION_H', axis = 1)\n",
    "    \n",
    "    # specify the number of lag games\n",
    "    n_games = 8\n",
    "    n_features = 53\n",
    "    \n",
    "    data_X = data.iloc[:, :-1]\n",
    "    data_y = np.array(data['WL_H'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    data_X = scaler.fit_transform(data_X)\n",
    "\n",
    "    scaled = np.hstack((data_X, data_y.reshape(data.shape[0], 1)))\n",
    "    \n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n_games, 1)\n",
    "    # print(reframed.shape)\n",
    "\n",
    "    # split into train and test sets\n",
    "    values = reframed.values\n",
    "    train = values[:int(reframed.shape[0]*0.8), :]\n",
    "    test = values[int(reframed.shape[0]*0.8):, :]\n",
    "    \n",
    "    # print(df.shape[0], df.shape[0]*0.8)\n",
    "    # print(test)\n",
    "    \n",
    "    # split into input and outputs\n",
    "    n_obs = n_games * n_features\n",
    "    train_X, train_y = train[:, :n_obs], train[:, -1]\n",
    "    test_X, test_y = test[:, :n_obs], test[:, -1]\n",
    "    # print(train_X.shape, len(train_X), train_y.shape)\n",
    "    # print(test_X.shape, len(test_X), test_y.shape)\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_games, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_games, n_features))\n",
    "    # print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    \n",
    "    # print(train_y)\n",
    "    # print(test_y)\n",
    "    \n",
    "    history = model.fit(train_X, train_y,\n",
    "                    callbacks = [es],\n",
    "                    epochs=5000, \n",
    "                    batch_size=10,\n",
    "                    validation_data=(test_X, test_y),\n",
    "                    verbose = 1,\n",
    "                    shuffle = False)\n",
    "    \n",
    "    val_accuracies.append(history.history['val_accuracy'])\n",
    "    train_accuracies.append(history.history['accuracy'])\n",
    "    val_losses.append(history.history['val_loss'])\n",
    "    train_losses.append(history.history['loss'])\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1561c6a-f5a8-47c1-bd3d-b423cceaeeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"{(end-start)//60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4416073b-0181-4deb-8417-6bf072ccf0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nba_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('nba_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33578c17-7354-4c1b-a380-4fbf58dd3d38",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0e480ac9-d003-423b-b8c4-115640a96449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b7391-f2bf-4148-9350-5271fb9df4d7",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dde67b87-c46b-4fd7-8442-795ab6aec4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accuracies = [item for sublist in val_accuracies for item in sublist]\n",
    "train_accuracies = [item for sublist in train_accuracies for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "814572a4-bc7d-4431-8f4b-02aa7ebf8f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([train_accuracies, val_accuracies])\n",
    "plt.xticks([1, 2], ['Train Accuracy', 'Validation Accuracy'])\n",
    "plt.title('Train vs Validation Accuracy Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "098ef55a-3501-4fb9-8c34-ebca3193b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy Mean: 0.7712123248450182\n",
      "Validation Accuracy Median: 0.7644444704055786\n"
     ]
    }
   ],
   "source": [
    "print(f\"Validation Accuracy Mean: {np.mean(val_accuracies)}\")\n",
    "print(f\"Validation Accuracy Median: {np.median(val_accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395cdc7b-1105-45f3-8632-b488c8143e33",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "054f2e59-5fd6-4f31-b989-97044c48d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = [item for sublist in val_losses for item in sublist]\n",
    "train_losses = [item for sublist in train_losses for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8af840bb-e8a1-4c23-979f-fc553c8ba1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot([train_losses, val_losses])\n",
    "plt.xticks([1, 2], ['Train Loss', 'Validation Loss'])\n",
    "plt.title('Train vs Validation Loss Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51bae6-838f-4566-8ce5-a2168789f514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
